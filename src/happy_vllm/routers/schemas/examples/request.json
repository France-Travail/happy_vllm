{
    "generate": {
        "Nominal case": {
            "summary": "Nominal case",
            "description": "The nominal case where we we use all default values",
            "value": {
                "prompt": "This is a prompt example. Please complete it with a joke. JOKE:"
            }
        },
        "simple_case": {
            "summary": "Simple case",
            "description": "A simple case where we use some vLLM sampling parameters",
            "value": {
                "prompt": "This is a prompt example. Please complete it with a joke. JOKE:",
                "repetition_penalty": 1.1,
                "temperature": 0.5,
                "top_p": 0.8,
                "max_tokens": 100
            }
        },
        "min_tokens": {
            "summary": "Use of min_tokens",
            "description": "How to use `min_tokens`. Note that `min_tokens` must be inferior or equal to `max_tokens`. It is incompatible with response_pool. The response of the LLM will be at least 40 tokens long.",
            "value": {
                "prompt": "This is a prompt example. Please complete it with a joke. JOKE:",
                "repetition_penalty": 1.1,
                "temperature": 0.5,
                "top_p": 0.8,
                "min_tokens": 40,
                "max_tokens": 100
            }
        },
        "response_pool": {
            "summary": "Use of response_pool",
            "description": "How to use `response_pool`. Note that `response_pool` is incompatible with `min_tokens` and `json_format`. In this example, the LLM will answer one of the three proposed responses, even if the choices are not present in the prompt",
            "value": {
                "prompt": "Was was the occupation of Ada Lovelace ?",
                "temperature": 0,
                "response_pool": [
                    "mathematician",
                    "astronaut",
                    "show writer"
                ]
            }
        },
        "json_format_json_simple": {
            "summary": "Use of json_format with a simple json",
            "description": "How to use `json_format` with a simple json. Note that `json_format` is incompatible with `response_pool`. In this example, we are not obligated to put in the prompt what json we are expecting even if it may help the LLM answer correctly. The keyword `json_format_is_json_schema` should be set to false since we are using a simple json but is not mandatory since it is the default value.",
            "value": {
                "prompt": "Describe Ada Lovelace. Your answer should be in form of a json with the keywords name, age, is_alive, height_in_meters, names_of_children.",
                "temperature": 0,
                "repetition_penalty": 1,
                "top_p": 0.8,
                "max_tokens": 100,
                "json_format": {
                    "name": "string",
                    "age": "integer",
                    "is_alive": "boolean",
                    "height_in_meters": "number",
                    "names_of_children": [
                        "string"
                    ]
                },
                "json_format_is_json_schema": false
            }
        },
        "json_format_json_schema": {
            "summary": "Use of json_format with a json schema",
            "description": "How to use `json_format` with a json schema. Note that `json_format` is incompatible with `response_pool`. In this example, we are not obligated to put in the prompt what json we are expecting even if it may help the LLM answer correctly. The keyword `json_format_is_json_schema` should be set to true since we are using a json schema. For more complex json schema, please read the corresponding documentation : https://json-schema.org/",
            "value": {
                "prompt": "Describe Ada Lovelace. Your answer should be in form of a json with the keywords name, age, is_alive, height_in_meters, names_of_children.",
                "temperature": 0,
                "repetition_penalty": 1,
                "top_p": 0.8,
                "max_tokens": 100,
                "json_format": {
                    "type": "object",
                    "properties": {
                        "name": {
                            "type": "string"
                        },
                        "age": {
                            "type": "integer"
                        },
                        "is_alive": {
                            "type": "boolean"
                        },
                        "height_in_meters": {
                            "type": "number"
                        },
                        "names_of_children": {
                            "type": "array",
                            "items": {
                                "type": "string"
                            }
                        }
                    },
                    "required": [
                        "name",
                        "age",
                        "is_alive",
                        "height_in_meters",
                        "names_of_children"
                    ]
                },
                "json_format_is_json_schema": true
            }
        }
    },
    "generate_stream": {
        "Nominal case": {
            "summary": "Nominal case",
            "description": "The nominal case where we we use all default values",
            "value": {
                "prompt": "This is a prompt example. Please complete it with a joke. JOKE:"
            }
        },
        "simple_case": {
            "summary": "Simple case",
            "description": "A simple case where we use some vLLM sampling parameters",
            "value": {
                "prompt": "This is a prompt example. Please complete it with a joke. JOKE:",
                "repetition_penalty": 1.1,
                "temperature": 0.5,
                "top_p": 0.8,
                "max_tokens": 100
            }
        },
        "min_tokens": {
            "summary": "Use of min_tokens",
            "description": "How to use `min_tokens`. Note that `min_tokens` must be inferior or equal to `max_tokens`. It is incompatible with response_pool. The response of the LLM will be at least 40 tokens long.",
            "value": {
                "prompt": "This is a prompt example. Please complete it with a joke. JOKE:",
                "repetition_penalty": 1.1,
                "temperature": 0.5,
                "top_p": 0.8,
                "min_tokens": 40,
                "max_tokens": 100
            }
        },
        "response_pool": {
            "summary": "Use of response_pool",
            "description": "How to use `response_pool`. Note that `response_pool` is incompatible with `min_tokens` and `json_format`. In this example, the LLM will answer one of the three proposed responses, even if the choices are not present in the prompt",
            "value": {
                "prompt": "Was was the occupation of Ada Lovelace ?",
                "temperature": 0,
                "response_pool": [
                    "mathematician",
                    "astronaut",
                    "show writer"
                ]
            }
        },
        "json_format_json_simple": {
            "summary": "Use of json_format with a simple json",
            "description": "How to use `json_format` with a simple json. Note that `json_format` is incompatible with `response_pool`. In this example, we are not obligated to put in the prompt what json we are expecting even if it may help the LLM answer correctly. The keyword `json_format_is_json_schema` should be set to false since we are using a simple json but is not mandatory since it is the default value.",
            "value": {
                "prompt": "Describe Ada Lovelace. Your answer should be in form of a json with the keywords name, age, is_alive, height_in_meters, names_of_children.",
                "temperature": 0,
                "repetition_penalty": 1,
                "top_p": 0.8,
                "max_tokens": 100,
                "json_format": {
                    "name": "string",
                    "age": "integer",
                    "is_alive": "boolean",
                    "height_in_meters": "number",
                    "names_of_children": [
                        "string"
                    ]
                },
                "json_format_is_json_schema": false
            }
        },
        "json_format_json_schema": {
            "summary": "Use of json_format with a json schema",
            "description": "How to use `json_format` with a json schema. Note that `json_format` is incompatible with `response_pool`. In this example, we are not obligated to put in the prompt what json we are expecting even if it may help the LLM answer correctly. The keyword `json_format_is_json_schema` should be set to true since we are using a json schema. For more complex json schema, please read the corresponding documentation : https://json-schema.org/",
            "value": {
                "prompt": "Describe Ada Lovelace. Your answer should be in form of a json with the keywords name, age, is_alive, height_in_meters, names_of_children.",
                "temperature": 0,
                "repetition_penalty": 1,
                "top_p": 0.8,
                "max_tokens": 100,
                "json_format": {
                    "type": "object",
                    "properties": {
                        "name": {
                            "type": "string"
                        },
                        "age": {
                            "type": "integer"
                        },
                        "is_alive": {
                            "type": "boolean"
                        },
                        "height_in_meters": {
                            "type": "number"
                        },
                        "names_of_children": {
                            "type": "array",
                            "items": {
                                "type": "string"
                            }
                        }
                    },
                    "required": [
                        "name",
                        "age",
                        "is_alive",
                        "height_in_meters",
                        "names_of_children"
                    ]
                },
                "json_format_is_json_schema": true
            }
        }
    },
    "tokenizer": {
        "nominal_case": {
            "summary": "Nominal case",
            "description": "This is a nominal case. For more details on what is the vanilla version of the tokenizer or not, please read the documentation : https://france-travail.github.io/happy_vllm/",
            "value": {
                "text": "This is a text example",
                "with_tokens_str": true,
                "vanilla": true
            }
        }
    },
    "vllm_tokenizer": {
        "completions": {
            "summary": "Completion",
            "description": "Completion case from vllm tokenizer request, check https://github.com/vllm-project/vllm/blob/1a36287b89f337057ebeb5d1bee30567e985b444/vllm/entrypoints/openai/protocol.py#L729C1-L729C17",
            "value": {
                "model": "my_model",
                "prompt": "This is a text example",
                "add_special_tokens": true
            }
        },
        "chat_completions": {
            "summary": "Chat completion",
            "description": "Chat completion case from vllm tokenizer request, check https://github.com/vllm-project/vllm/blob/1a36287b89f337057ebeb5d1bee30567e985b444/vllm/entrypoints/openai/protocol.py#L729C1-L729C17",
            "value": {
                "model": "my_model",
                "messages": [
                    "This is a text example",
                    "and another"
                ],
                "add_special_tokens": true,
                "add_generation_prompt": true
            }
        }
    },
    "decode": {
        "nominal_case": {
            "summary": "Nominal case",
            "description": "Decode a list of token ids. It is the inverse of the endpoint tokenizer. For more details on what is the vanilla version of the tokenizer or not, please read the documentation : https://france-travail.github.io/happy_vllm/",
            "value": {
                "token_ids": [
                    1,
                    17162,
                    28725,
                    910,
                    460,
                    368,
                    1550
                ],
                "with_tokens_str": true,
                "vanilla": true
            }
        }
    },
    "vllm_decode": {
        "nominal_case": {
            "summary": "Nominal case",
            "description": "Decode a list of token ids. It is the inverse of the endpoint tokenizer. For more details on what is the vanilla version of the tokenizer or not, please read the documentation : https://france-travail.github.io/happy_vllm/",
            "value": {
                "tokens": [
                    "1",
                    "17162",
                    "28725",
                    "910",
                    "460",
                    "368",
                    "1550"
                ],
                "model": "my_model"
            }
        }
    },
    "split_text": {
        "nominal_case": {
            "summary": "Nominal case",
            "description": "In this example, the separators are the default separators.",
            "value": {
                "text": "Hey, how are you ? I am clearly fine. And you ? Exceptionally good, thanks for asking.",
                "num_tokens_in_chunk": 4,
                "separators": [".", "!", "?", "|", " .", " !", " ?", " |"]
              }
        }
    },
    "metadata_text": {
        "nominal_case": {
            "summary": "Nominal case",
            "description": "A nominal case",
            "value": {
                "text": "Hey, how are you ?",
                "truncation_side": "left",
                "max_length": 2
              }
        }
    },
    "completions": {
        "Nominal case": {
            "summary": "Nominal case",
            "description": "The nominal case where we we use all default values",
            "value": {
                "prompt": "This is a prompt example. Please complete it with a joke. JOKE:",
                "model": "my_model"
            }
        },
        "simple_case": {
            "summary": "Simple case",
            "description": "A simple case where we use some vLLM sampling parameters",
            "value": {
                "prompt": "This is a prompt example. Please complete it with a joke. JOKE:",
                "model": "my_model",
                "repetition_penalty": 1.1,
                "temperature": 0.5,
                "top_p": 0.8,
                "max_tokens": 100
            }
        },
        "choices": {
            "summary": "Use of guided choices",
            "description": "How to use `guided_choice`. In this example, the LLM will answer one of the three proposed responses, even if the choices are not present in the prompt",
            "value": {
                "prompt": "Was was the occupation of Ada Lovelace ?",
                "model": "my_model",
                "temperature": 0,
                "guided_choice": [
                    "mathematician",
                    "astronaut",
                    "show writer"
                ]
            }
        },
        "guided_json": {
            "summary": "Use of guided_json with a json schema",
            "description": "How to use `guided_json` with a json schema. In this example, we are not obligated to put in the prompt what json we are expecting even if it may help the LLM answer correctly. For more complex json schema, please read the corresponding documentation : https://json-schema.org/",
            "value": {
                "prompt": "Describe Ada Lovelace. Your answer should be in form of a json with the keywords name, age, is_alive, height_in_meters, names_of_children.",
                "model": "my_model",
                "temperature": 0,
                "repetition_penalty": 1,
                "top_p": 0.8,
                "max_tokens": 100,
                "guided_json": {
                    "type": "object",
                    "properties": {
                        "name": {
                            "type": "string"
                        },
                        "age": {
                            "type": "integer"
                        },
                        "is_alive": {
                            "type": "boolean"
                        },
                        "height_in_meters": {
                            "type": "number"
                        },
                        "names_of_children": {
                            "type": "array",
                            "items": {
                                "type": "string"
                            }
                        }
                    },
                    "required": [
                        "name",
                        "age",
                        "is_alive",
                        "height_in_meters",
                        "names_of_children"
                    ]
                }
            }
        }
    },
    "chat_completions": {
        "nominal_case": {
            "summary": "Nominal case",
            "description": "A nominal case",
            "value": {
                "messages": [{
                    "role": "system",
                    "content": "You are a helpful assistant."
                }, {
                    "role": "user",
                    "content": "Who won the world series in 2020?"
                }, {
                    "role":
                    "assistant",
                    "content":
                    "The Los Angeles Dodgers won the World Series in 2020."
                }, {
                    "role": "user",
                    "content": "Where was it played?"
                }],
                "model": "my_model"
              }
        }
    }
}