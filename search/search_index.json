{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Happy_vLLM","text":"<p>Happy_vLLM is a REST API, production ready. It is based on the popular library vLLM and provide an API for it.</p>"},{"location":"#installation","title":"Installation","text":"<p>You can install happy_vLLM using pip:</p> <pre><code>pip install happy_vllm\n</code></pre> <p>Or build it from source:</p> <pre><code>git clone https://github.com/France-Travail/happy_vllm.git\ncd happy_vllm\npip install -e .\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Just use the entrypoint <code>happy-vllm</code> (see arguments for a list of all possible arguments)</p> <pre><code>happy-vllm --model path_to_model --host 127.0.0.1 --port 5000 --model-name my_model\n</code></pre> <p>It will launch the API and you can directly query it for example with </p> <pre><code>curl 127.0.0.1:5000/v1/info\n</code></pre> <p>To get various information on the application or </p> <pre><code>curl 127.0.0.1:5000/v1/completions -d '{\"prompt\": \"Hey,\", \"model\": \"my_model\"}'\n</code></pre> <p>if you want to generate your first LLM response using happy_vLLM. See endpoints for more details on all the endpoints provided by happy_vLLM. </p>"},{"location":"#deploy-with-docker-image","title":"Deploy with Docker image","text":"<p>A docker image is available from the Github Container Registry :  </p> <p><pre><code>docker pull ghcr.io/france-travail/happy_vllm:latest\n</code></pre> See deploying_with_docker for more details on how to serve happy_vLLM with docker. </p>"},{"location":"#swagger","title":"Swagger","text":"<p>You can reach the swagger UI at the <code>/docs</code> endpoint (so for example by default at <code>127.0.0.1:5000/docs</code>). You will be provided all the endpoints and examples on how to use them.</p>"},{"location":"arguments/","title":"Arguments","text":""},{"location":"arguments/#cli-environment-variables-and-env","title":"CLI, environment variables and .env","text":"<p>Arguments for launching happy_vLLM are all defined in <code>utils_args.py</code>. They can be defined via three methods which are by order of priority:</p> <ul> <li>directly from the cli (for example using the entrypoint <code>happy-vllm --model path_to_model</code>)</li> <li>from environment variables (for example <code>export MODEL=\"path_to_model\"</code> and then use the entrypoint <code>happy-vllm</code>)</li> <li>from a <code>.env</code> file located where you use the <code>happy-vllm</code> entrypoint (see the <code>.env.example</code> provided for a complete description)</li> </ul> <p>Note that in the cli, the arguments are lowercase and with hyphens <code>-</code> whereas in the environment variables and <code>.env</code>, they are uppercase and with underscore <code>_</code>. So for example the argument <code>max-model-len</code> in the cli corresponds to <code>MAX_MODEL_LEN</code> in the environment variables or <code>.env</code> file.</p> <p>The order of priority means that the arguments entered via the cli will always overwrite the argument from environment variables and <code>.env</code> file if it is also defined there. Similarly, the environment variables will always trump the variables from a <code>.env</code> file.</p>"},{"location":"arguments/#arguments-definition","title":"Arguments definition","text":""},{"location":"arguments/#application-arguments","title":"Application arguments","text":"<p>Here is a list of arguments useful for the application (they all have default values and are optional):</p> <ul> <li><code>host</code> : The name of the host (default value is <code>127.0.0.1</code>)</li> <li><code>port</code> : The port number (default value is <code>5000</code>)</li> <li><code>model-name</code> : The name of the model which will be given by the <code>/v1/info</code> endpoint or the <code>/v1/models</code>. Knowing the name of the model is important to be able to use the endpoints <code>/v1/completions</code> and <code>/v1/chat/completions</code> (default value is <code>?</code>)</li> <li><code>extra-information</code> : The path to a json which will be added to the <code>/v1/info</code> endpoint in the <code>extra_information</code> field </li> <li><code>app-name</code>: The name of the application (default value is <code>happy_vllm</code>)</li> <li><code>api-endpoint-prefix</code>: The prefix added to all the API endpoints (default value is no prefix)</li> <li><code>explicit-errors</code>: If <code>False</code>, the message displayed when an <code>500 error</code> is encountered will be <code>Internal Server Error</code>. If <code>True</code>, the message displayed will be more explicit and give information on the underlying error. The <code>True</code> setting is not recommended in a production setting (default value is <code>False</code>).</li> <li><code>allow-credentials</code>: The CORS setting (default value is <code>False</code>)</li> <li><code>allowed-origins</code>: The CORS setting (default value is <code>[\"*\"]</code>)</li> <li><code>allowed-methods</code>: The CORS setting (default value is <code>[\"*\"]</code>)</li> <li><code>allowed-headers</code>: The CORS setting (default value is <code>[\"*\"]</code>)</li> <li><code>uvicorn-log-level</code>: The log level of uvicorn (default value is <code>info</code>)</li> <li><code>ssl-keyfile</code>: Uvicorn setting, the file path to the SSL key file (default value is <code>None</code>)</li> <li><code>ssl-certfile</code>: Uvicorn setting, the file path to the SSL cert file (default value is <code>None</code>)</li> <li><code>ssl-ca-certs</code>: Uvicorn setting, the CA certificates file (default value is <code>None</code>)</li> <li><code>enable-ssl-refresh</code>: Refresh SSL Context when SSL certificate files change (default value is <code>False</code>)</li> <li><code>ssl-cert-reqs</code>: Uvicorn setting, Whether client certificate is required (see stdlib ssl module's) (default value is <code>0</code>)</li> <li><code>root_path</code>: The FastAPI root path (default value is <code>None</code>)</li> <li><code>lora-modules</code>: LoRA module configurations in the format name=path</li> <li><code>chat-template</code>: The file path to the chat template, or the template in single-line form for the specified model (see the documentation of vLLM for more details). Useful in the <code>/v1/chat/completions</code> endpoint</li> <li><code>chat-template-content-format</code>: The format to render message content within a chat template. <code>string</code> will render the content as a string. 'Example: \"Hello World\"', <code>openai</code> will render the content as a list of dictionaries, similar to OpenAI schema. 'Example: [{\"type\": \"text\", \"text\": \"Hello world!\"}]' (default value is <code>auto</code>)</li> <li><code>response-role</code>: The role name to return if <code>request.add_generation_prompt=true</code>. Useful in the <code>/v1/chat/completions</code> endpoint</li> <li><code>with-launch-arguments</code>: Whether the route <code>/v1/launch_arguments</code> gives the launch arguments or an empty json (default value is <code>False</code>)</li> <li><code>return-tokens-as-token-ids</code>: \"When <code>--max-logprobs</code>  is specified, represents single tokens as strings of the form 'token_id:{token_id}' so that tokens that are not JSON-encodable can be identified (default value is <code>False</code>)</li> <li><code>max-log-len</code>: Max number of prompt characters or prompt ID numbers being printed in log (default value is unlimited)</li> <li><code>prompt-adapters</code>: Prompt adapter configurations in the format name=path. Multiple adapters can be specified (default value is <code>None</code>)</li> <li><code>disable-frontend-multiprocessing</code>: If specified, will run the OpenAI frontend server in the same process as the model serving engine (default value is <code>False</code>)</li> <li><code>enable-request-id-headers</code>: If specified, API server will add X-Request-Id header to responses. Caution: this hurts performance at high QPS (default value <code>False</code>)</li> <li><code>enable-auto-tool-choice</code>: Enable auto tool choice for supported models. Use --tool-call-parser\" \"to specify which parser to use\" (default value is <code>False</code>)</li> <li><code>tool-call-parser</code>: Select the tool call parser depending on the model that you're using. This is used to parse the model-generated tool call. Required for --enable-auto-tool-choice. (default value is <code>None</code>, only <code>mistral</code> and <code>hermes</code> are allowed)</li> <li><code>tool-parser-plugin</code>: Special the tool parser plugin write to parse the model-generated tool into OpenAI API format, the name register in this plugin can be used in --tool-call-parser (default value is <code>\"\"</code>)</li> <li><code>disable-fastapi-docs</code>: Disable FastAPI's OpenAPI schema, Swagger UI, and ReDoc endpoint (default value is <code>False</code>)</li> <li><code>enable-prompt-tokens-details</code>: If set to True, enable prompt_tokens_details in usage (default value is <code>False</code>)</li> <li><code>enable-server-load-tracking</code>: If set to True, enable tracking server_load_metrics in the app state (default value is <code>False</code>)</li> <li><code>disable-uvicorn-access-log</code>: Disable uvicorn access log (default value is <code>False</code>)</li> </ul>"},{"location":"arguments/#model-arguments","title":"Model arguments","text":"<p>All the usual vLLM arguments for the model and vLLM itself are usable. They all have default values (defined by vLLM) and are optional. The exhaustive list is here (source code) or here (documentation). Here are some of them:</p> <ul> <li><code>model</code>: The path to the model or the huggingface repository</li> <li><code>task</code>: Possible choices: auto, generate, embedding, embed, classify, transcription. The task to use the model for. Each vLLM instance only supports one task, even if the same model can be used for multiple tasks. When the model only supports one task, \"auto\" can be used to select it; otherwise, you must specify explicitly which task to use. If unspecified, will use the default value of <code>auto</code>.</li> <li><code>dtype</code>: The data type for model weights and activations.</li> <li><code>max-model-len</code>: The model context length. If unspecified, will be automatically derived from the model.</li> <li><code>gpu-memory-utilization</code>: The fraction of GPU memory to be used for the model executor, which can range from 0 to 1. If unspecified, will use the default value of 0.9.</li> <li><code>config-format</code>: Possible choices: auto, hf, mistral. The format of the model config to load. If unspecified, will use the default value of <code>ConfigFormat.AUTO</code>.</li> <li><code>load-format</code>: Possible choices: auto, pt, safetensors, npcache, dummy, tensorizer, sharded_state, gguf, bitsandbytes, mistral, runai_streamer. The format of the model weights to load. If unspecified, will use the default value of <code>auto</code>.</li> <li><code>tokenizer-mode</code>:Possible choices: auto, slow, mistral, custom. The tokenizer mode. If unspecified, will use the default value of <code>auto</code>.</li> </ul>"},{"location":"deploying_with_docker/","title":"Deploying with docker","text":"<p>The docker image is available on the Github Container Registry</p>"},{"location":"deploying_with_docker/#pull-the-image-from-github-container-registry","title":"Pull the image from Github Container Registry","text":"<pre><code>docker pull ghcr.io/france-travail/happy_vllm:latest\n</code></pre>"},{"location":"deploying_with_docker/#launch-a-container","title":"Launch a container","text":"<p><pre><code>docker run -it ghcr.io/france-travail/happy_vllm:latest --model mistralai/Mistral-7B-v0.1\n</code></pre> See arguments for more details the list of all arguments useful for the application and model for happy_vLLM. </p>"},{"location":"deploying_with_docker/#build-docker-image-from-source-via-the-provided-dockerfile","title":"Build docker image from source via the provided dockerfile","text":"<pre><code>docker build -t france-travail/happy_vllm:latest .\n</code></pre>"},{"location":"pros/","title":"What are the bonuses of using happy_vLLM ?","text":"<p>happy_vLLM provides several functionalities useful for a production purpose. Here are a few.</p>"},{"location":"pros/#environment-variables","title":"Environment variables","text":"<p>All the arguments used to launch the api and load the model can be specified via three methods : environment variables, .env files or cli arguments (see this section for more details). This way, you can easily specify the arguments according to your different environments : dev, pre-production, production, ...</p>"},{"location":"pros/#new-endpoints","title":"New endpoints","text":"<p>happy_vLLM add new endpoints useful for the users wich don't need to set up their own instances (for example for a tokenizer) but can directly use those provided by this API. For more details on endpoints, click here</p> <p>If you would like to see an endpoint added, don't hesitate to open an issue or a PR.</p>"},{"location":"pros/#swagger","title":"Swagger","text":"<p>A well documented swagger (the UI being reachable at the <code>/docs</code> endpoint) in order for users not so used to using API to be able to quickly get the hang of it and be as autonomous as possible in querying the LLM. </p>"},{"location":"pros/#benchmarks","title":"Benchmarks","text":"<p>We developped a library benchmark_llm_serving which provides a more complete benchmark of the vLLM serving API than the vanilla one.</p>"},{"location":"endpoints/data_manipulation/","title":"Data manipulation endpoints","text":"<p>In this section we will give more details on the endpoints <code>/v1/metadata_text</code> and <code>v1/split_text</code></p>"},{"location":"endpoints/data_manipulation/#v1metadata_text-post","title":"/v1/metadata_text (POST)","text":"<p>Returns the number of tokens of a text and indicates the part that would be truncated if too long. Note that this endpoint uses the special version of the tokenizer provided by happy_vLLM (more details here). The format of the input is as follows:</p> <pre><code>{\n  \"text\": \"Hey, how are you ?\",\n  \"truncation_side\": \"left\",\n  \"max_length\": 2\n}\n</code></pre> <ul> <li><code>text</code>: The text we want to analyze</li> <li><code>truncation_side</code>: The side of the truncation. This keyword is optional and the default value is the one of the tokenizer which can be obtained for example via the <code>/v1/info</code> endpoint</li> <li><code>max_length</code>: The maximal length of the string before the truncation acts. This keyword is optional and the default value is the <code>max_model_len</code> of the model which can be obtained for example via the <code>/v1/info</code> endpoint</li> </ul> <p>The format of the output is as follows:</p> <pre><code>{\n  \"tokens_nb\": 6,\n  \"truncated_text\": \"Hey, how are\"\n}\n</code></pre> <ul> <li><code>tokens_nb</code>: The number of tokens in the given text</li> <li><code>truncated_text</code>: The part of the text which would be truncated</li> </ul>"},{"location":"endpoints/data_manipulation/#v1split_text-post","title":"/v1/split_text (POST)","text":"<p>Splits a text in chunks. You can specify a minimal number of tokens present in each chunk. Each chunk will be delimited by separators you can specify. </p> <p>The format of the input is as follows:</p> <pre><code>{\n  \"text\": \"Hey, how are you ? I am clearly fine. And you ? Exceptionally good, thanks for asking.\",\n  \"num_tokens_in_chunk\": 4,\n  \"separators\": [\".\", \"!\", \"?\", \"|\", \" .\", \" !\", \" ?\", \" |\"]\n}\n</code></pre> <ul> <li><code>text</code>: The text to split</li> <li><code>num_tokens_in_chunk</code>: The minimal number of tokens you want in a chunk. The keyword is optional, the default value is 200.</li> <li><code>separators</code>: The list of separators which can separate the chunks. Note that they should be corresponding to tokens of the tokenizer. That's why it might be a good practice, depending on the specific tokenizer, to include a space in the separator (such as this : <code>?</code>). This keyword is optional. The default value is [\".\", \"!\", \"?\", \"|\", \" .\", \" !\", \" ?\", \" |\"]</li> </ul> <p>The format of the output is as follows:</p> <pre><code>{\n  \"split_text\": [\n    \"Hey, how are you ?\",\n    \" I am clearly fine.\",\n    \" And you ? Exceptionaly good, thanks for asking.\"\n  ]\n}\n</code></pre> <ul> <li><code>split_text</code>: The list of chunks obtained by splitting the text. Note that in this example, in the last chunk, even though \" ?\" is a separator, it was not split in two since \" And you ?\" is less than 4 tokens.</li> </ul>"},{"location":"endpoints/embeddings/","title":"Embeddings","text":""},{"location":"endpoints/embeddings/#embeddings-endpoints","title":"Embeddings endpoints","text":""},{"location":"endpoints/embeddings/#v1embeddings","title":"/v1/embeddings","text":"<p>Mirror of the <code>/v1/embeddings</code> endpoint of vLLM. Note that only this endpoint has been implemented in happy_vLLM (in particular <code>/pooling</code> and <code>/score</code> are not implemented). More details here. Note that happy_vLLM must be launched with the argument <code>--task embed</code> in order for this endpoint to be available.</p> <p>The format of the input is as follows :</p> <pre><code>{\n  \"input\": [\"First text\", \"Seconde text\"],\n  \"model\": \"my_model\",\n  \"encoding_format\": \"float\"\n}\n</code></pre> <p>And the output is of the form:</p> <pre><code>{\n  \"id\": \"embd-418265ed36ab48e5bd153ac6e9d33c24\",\n  \"object\": \"list\",\n  \"created\": 1736946607,\n  \"model\": \"my_model\",\n  \"data\": [\n    {\n      \"index\": 0,\n      \"object\": \"embedding\",\n      \"embedding\": [float_1, float_2, .....]\n      },\n      {\n      \"index\": 1,\n      \"object\": \"embedding\",\n      \"embedding\": [float_1, float_2, .....]}\n      ]\n}\n</code></pre>"},{"location":"endpoints/endpoints/","title":"Endpoints","text":"<p>happy_vLLM provides several endpoints which cover most of the use cases. Feel free to open an issue or a PR if you would like to add an endpoint. All these endpoints (except for the <code>/metrics</code> endpoint) are prefixed by, well, a prefix which by default is absent. </p>"},{"location":"endpoints/endpoints/#technical-endpoints","title":"Technical endpoints","text":""},{"location":"endpoints/endpoints/#v1info-get","title":"/v1/info (GET)","text":"<p>Provides information on the API and the model (more details here)</p>"},{"location":"endpoints/endpoints/#metrics-get","title":"/metrics (GET)","text":"<p>The technical metrics obtained for prometheus (more details here)</p>"},{"location":"endpoints/endpoints/#liveness-get","title":"/liveness (GET)","text":"<p>The liveness endpoint (more details here)</p>"},{"location":"endpoints/endpoints/#readiness-get","title":"/readiness (GET)","text":"<p>The readiness endpoint (more details here)</p>"},{"location":"endpoints/endpoints/#v1models-get","title":"/v1/models (GET)","text":"<p>The Open AI compatible endpoint used, for example, to get the name of the model. Mimicks the vLLM implementation (more details here)</p>"},{"location":"endpoints/endpoints/#v1launch_arguments-get","title":"/v1/launch_arguments (GET)","text":"<p>Gives all the arguments used when launching the application. <code>--with-launch-arguments</code> must be activated (more details here)</p>"},{"location":"endpoints/endpoints/#generating-endpoints","title":"Generating endpoints","text":""},{"location":"endpoints/endpoints/#v1completions-and-v1chatcompletions-post","title":"/v1/completions and /v1/chat/completions (POST)","text":"<p>These two endpoints mimick the ones of vLLM. They follow the Open AI contract and you can find more details in the vLLM documentation</p>"},{"location":"endpoints/endpoints/#v1abort_request-post","title":"/v1/abort_request (POST)","text":"<p>Aborts a running request </p>"},{"location":"endpoints/endpoints/#tokenizer-endpoints","title":"Tokenizer endpoints","text":""},{"location":"endpoints/endpoints/#v1tokenizer-post-deprecated","title":"/v1/tokenizer (POST)  Deprecated","text":"<p>Used to tokenizer a text (more details here)</p>"},{"location":"endpoints/endpoints/#v2tokenizer-post","title":"/v2/tokenizer (POST)","text":"<p>Used to tokenizer a text (more details here)</p>"},{"location":"endpoints/endpoints/#v1decode-post-deprecated","title":"/v1/decode (POST)  Deprecated","text":"<p>Used to decode a list of token ids (more details here)</p>"},{"location":"endpoints/endpoints/#v2decode-post","title":"/v2/decode (POST)","text":"<p>Used to decode a list of token ids (more details here)</p>"},{"location":"endpoints/endpoints/#data-manipulation-endpoints","title":"Data manipulation endpoints","text":""},{"location":"endpoints/endpoints/#v1metadata_text-post","title":"/v1/metadata_text (POST)","text":"<p>Used to know which part of a prompt will be truncated (more details here)</p>"},{"location":"endpoints/endpoints/#v1split_text-post","title":"/v1/split_text (POST)","text":"<p>Splits a text on some separators, for example to prepare for some RAG (more details here)</p>"},{"location":"endpoints/endpoints/#embeddings-endpoint","title":"Embeddings endpoint","text":""},{"location":"endpoints/endpoints/#v1embeddings","title":"/v1/embeddings","text":"<p>Used to obtain the embeddings of a text (more details here)</p>"},{"location":"endpoints/endpoints/#lora-endpoints","title":"Lora endpoints","text":""},{"location":"endpoints/endpoints/#v1load_lora_adapter-post","title":"/v1/load_lora_adapter (POST)","text":"<p>Load a specific Lora adapter (more details in vLLM documentation)</p>"},{"location":"endpoints/endpoints/#v1unload_lora_adapter-post","title":"/v1/unload_lora_adapter (POST)","text":"<p>Unload a Lora adapter (more details in vLLM documentation)</p>"},{"location":"endpoints/endpoints/#transcription-endpoints","title":"Transcription endpoints","text":""},{"location":"endpoints/endpoints/#v1audiotranscriptions","title":"/v1/audio/transcriptions","text":"<p>Used to obtain the transcription of an audio file (more details here)</p>"},{"location":"endpoints/technical/","title":"Technical endpoints","text":"<p>Here we present the various technical endpoints : <code>/v1/info</code>, <code>/metrics</code>, <code>/liveness</code> and  <code>/readiness</code>, <code>/v1/models</code></p>"},{"location":"endpoints/technical/#v1info-get","title":"/v1/info (GET)","text":"<p>This endpoints gives various information on the application and the model. The format of the output is as follows:</p> <p><pre><code>{\n  \"application\": \"happy_vllm\",\n  \"version\": \"1.0.0\",\n  \"vllm_version\": \"0.4.0.post1\",\n  \"model_name\": \"The best LLM\",\n  \"truncation_side\": \"right\",\n  \"max_length\": 32768,\n  \"extra_information\": {}\n}\n</code></pre> In order to add a non empty dictionnary to the field \"extra_information\", you should pass the path to .json when initiating the API via the <code>--extra-information</code> argument.</p>"},{"location":"endpoints/technical/#metrics-get","title":"/metrics (GET)","text":"<p>We remind you that this endpoint is the only one not prefixed by the <code>api_endpoint_prefix</code>. This endpoint is generated by a prometheus client. To have more details on which metrics are available, please refer to vLLM documentation</p>"},{"location":"endpoints/technical/#liveness-get","title":"/liveness (GET)","text":"<p>Checks if the API is live. The format of the output is as follows:</p> <pre><code>{\n  \"alive\": \"ok\"\n}\n</code></pre>"},{"location":"endpoints/technical/#readiness-get","title":"/readiness (GET)","text":"<p>Checks if the API is ready. The format of the output is as follows:</p> <pre><code>{\n  \"ready\": \"ok\"\n}\n</code></pre> <p>If the API is not ready, the value is \"ko\"</p>"},{"location":"endpoints/technical/#v1models-get","title":"/v1/models (GET)","text":"<p>The Open AI compatible endpoint used, for example, to get the name of the model. Mimicks the vLLM implementation. </p>"},{"location":"endpoints/technical/#v1launch_arguments-get","title":"/v1/launch_arguments (GET)","text":"<p>If when launched the arguments <code>--with-launch-arguments</code> is given, this route gives all the parameters given when launching the application. In particular, the arguments given to the vLLM engine. It is useful for the benchmarks done by benchmark_llm_serving</p>"},{"location":"endpoints/tokenizer/","title":"Tokenizer","text":""},{"location":"endpoints/tokenizer/#tokenizer-endpoints","title":"Tokenizer endpoints","text":"<p>The tokenizer endpoints allow to use the tokenizer underlying the model. These endpoints are <code>/v2/tokenizer</code> and <code>/v2/decode</code> and you can find more details on each below.</p> <p> Deprecated These endpoints <code>/v1/tokenizer</code> and <code>/v1/decode</code> are deprecated</p>"},{"location":"endpoints/tokenizer/#v2tokenizer-post","title":"/v2/tokenizer (POST)","text":"<p>Tokenizes the given text. The format of the input is as follows according to the method:</p>"},{"location":"endpoints/tokenizer/#completions","title":"Completions","text":"<pre><code>{\n  \"model\": \"my_model\",\n  \"prompt\": \"This is a text example\",\n  \"add_special_tokens\": true\n}\n</code></pre> <ul> <li><code>model</code>: ID of the model to use</li> <li><code>prompt</code> : The text to tokenize</li> <li><code>add_special_tokens</code> : Add a special tokens to the begin (optional, default value : <code>true</code>)</li> </ul>"},{"location":"endpoints/tokenizer/#chatcompletions","title":"Chat/Completions","text":"<pre><code>{\n  \"model\": \"my_model\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"This is an example\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"This is an example\"\n    }\n  ],\n  \"add_special_tokens\": true,\n  \"add_generation_prompt\": true\n}\n</code></pre> <ul> <li><code>model</code> : ID of the model to use</li> <li><code>messages</code> : The texts to tokenize</li> <li><code>add_special_tokens</code> : Add a special tokens to the begin (optional, default value : <code>false</code>)</li> <li><code>add_generation_prompt</code> : Add generation prompt's model in decode response (optional, default value : <code>true</code>)</li> </ul> <p>The format of the output is as follows :</p> <pre><code>{\n  \"count\": [\n    23\n  ],\n  \"max_model_len\": 8192,\n  \"tokens\": [128000, 2028, 374, 264, 1495, 318]\n}\n</code></pre> <ul> <li><code>count</code>: The number of token in the input</li> <li><code>max_model_len</code>: Max model length in config</li> <li><code>tokens</code>: The list of token ids given by the tokenizer (give one extra token only if <code>add_special_tokens</code> was set to <code>true</code> in the request)</li> </ul>"},{"location":"endpoints/tokenizer/#v2decode-post","title":"/v2/decode (POST)","text":"<p>Decodes the given token ids. The format of the input is as follows :</p> <pre><code>{\n  \"tokens\": [128000, 2028, 374, 264, 1495, 318],\n  \"model\": \"my_model\"\n}\n</code></pre> <ul> <li><code>tokens</code>: The ids of the tokens we want to decode</li> <li><code>model</code>: ID of the model to use</li> </ul> <p>The format of the output is as follows:</p> <pre><code>{\n  \"prompt\": \"&lt;s&gt; Hey, how are you ?\"\n}\n</code></pre> <ul> <li><code>prompt</code>: The decoded string corresponding to the token ids</li> </ul>"},{"location":"endpoints/tokenizer/#v1tokenizer-post-deprecated","title":"/v1/tokenizer (POST)  Deprecated","text":"<p>Tokenizes the given text. The format of the input is as follows :</p> <pre><code>{\n  \"text\": \"text to tokenizer\",\n  \"with_tokens_str\": true,\n  \"vanilla\": true\n}\n</code></pre> <ul> <li><code>text</code>: The text to tokenize</li> <li><code>with_tokens_str</code>: Whether the list of tokens in string form should be given in the response (optional, default value : <code>false</code>)</li> <li><code>vanilla</code>: Whether we should use the vanilla version of the tokenizer or the happy_vLLM version (see this section for more details). This keyword is optional and the default value is <code>true</code>.</li> </ul> <p>The format of the output is as follows :</p> <pre><code>{\n  \"tokens_ids\": [\n    1,\n    17162,\n    28725,\n    910,\n    460,\n    368,\n    1550\n  ],\n  \"tokens_nb\": 7,\n  \"tokens_str\": [\n    \"&lt;s&gt;\",\n    \"\u2581Hey\",\n    \",\",\n    \"\u2581how\",\n    \"\u2581are\",\n    \"\u2581you\",\n    \"\u2581?\"\n  ]\n}\n</code></pre> <ul> <li><code>tokens_ids</code>: The list of token ids given by the tokenizer</li> <li><code>tokens_nb</code>: The number of tokens in the input</li> <li><code>tokens_str</code>: The string representation of each token (given only if <code>with_tokens_str</code> was set to <code>true</code> in the request)</li> </ul>"},{"location":"endpoints/tokenizer/#v1decode-post-deprecated","title":"/v1/decode (POST)  Deprecated","text":"<p>Decodes the given token ids. The format of the input is as follows :</p> <pre><code>{\n  \"token_ids\": [\n    1,\n    17162,\n    28725,\n    910,\n    460,\n    368,\n    1550\n  ],\n  \"with_tokens_str\": true,\n  \"vanilla\": true\n}\n</code></pre> <ul> <li><code>token_ids</code>: The ids of the tokens we want to decode</li> <li><code>with_tokens_str</code>: Whether we want the response to also decode the ids, id by id</li> <li><code>vanilla</code>: Whether we should use the vanilla version of the tokenizer or the happy_vLLM version (see this section for more details). This keyword is optional and the default value is <code>true</code>.</li> </ul> <p>The format of the output is as follows:</p> <pre><code>{\n  \"decoded_string\": \"&lt;s&gt; Hey, how are you ?\",\n  \"tokens_str\": [\n    \"&lt;s&gt;\",\n    \"\u2581Hey\",\n    \",\",\n    \"\u2581how\",\n    \"\u2581are\",\n    \"\u2581you\",\n    \"\u2581?\"\n  ]\n}\n</code></pre> <ul> <li><code>decoded_string</code>: The decoded string corresponding to the token ids</li> <li>\u0300<code>tokens_str</code>: The decoded string for each token id (given only if <code>with_tokens_str</code> was set to <code>true</code> in the request)</li> </ul>"},{"location":"endpoints/tokenizer/#vanilla-tokenizer-vs-happy_vllm-tokenizer","title":"Vanilla tokenizer vs happy_vLLM tokenizer","text":"<p>Using the endpoints <code>/v1/tokenizer</code> and <code>/v1/decode</code>, you can decide if you want to use the usual version of the tokenizers (with the keyword <code>vanilla</code> set to <code>true</code>). But in some cases, the tokenizer introduces special characters instead of whitespaces, adds a whitespace in front of the string etc. While it is usually the correct way to use the tokenizer (since the models have been trained with these), in some cases, you might want just to get rid of all these additions. We provide a simple way to do so just by setting the keyword <code>vanilla</code> to <code>false</code> in the endpoints <code>/v1/tokenizer</code> and <code>/v1/decode</code>.</p> <p>For example, if you want to encode and decode the string : <code>Hey, how are you ? Fine thanks.</code> with the Llama tokenizer, it will create the following tokens (in string forms) : </p> <p>For the usual tokenizer:</p> <p><code>[\"&lt;s&gt;\", \"\u2581Hey\", \",\", \"\u2581how\", \"\u2581are\", \"\u2581you\", \"\u2581?\", \"\u2581Fine\", \"\u2581thanks\", \".\"]</code> </p> <p>For the happy_vLLM tokenizer:</p> <p><code>[\"H\", \"ey\", \",\", \" how\", \" are\", \" you\", \" ?\", \" Fine\", \" thanks\", \".\"]</code></p> <p>Note that the \"Hey\" is not treated the same way, that the whitespaces are directly translated in real whitespaces and there is no initial whitespace.</p> <p>Note that our modified version of the tokenizer is the one used in the <code>/v1/metadata_text</code> endpoint (see this section for more details).</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>happy_vllm<ul> <li>application</li> <li>core<ul> <li>config</li> <li>logtools</li> <li>resources</li> </ul> </li> <li>engine<ul> <li>mp_engine</li> </ul> </li> <li>launch</li> <li>middlewares<ul> <li>exception</li> </ul> </li> <li>model<ul> <li>model_base</li> </ul> </li> <li>routers<ul> <li>functional</li> <li>schemas<ul> <li>functional</li> <li>technical</li> <li>utils</li> </ul> </li> <li>technical</li> </ul> </li> <li>utils</li> <li>utils_args</li> </ul> </li> </ul>"},{"location":"reference/happy_vllm/","title":"happy_vllm","text":""},{"location":"reference/happy_vllm/application/","title":"application","text":""},{"location":"reference/happy_vllm/application/#happy_vllm.application.declare_application","title":"<code>declare_application(async_engine_client, args)</code>  <code>async</code>","text":"<p>Create the FastAPI application</p> <p>See https://fastapi.tiangolo.com/tutorial/first-steps/ to learn how to customize your FastAPI application</p> Source code in <code>happy_vllm/application.py</code> <pre><code>async def declare_application(async_engine_client: MQLLMEngineClient, args: Namespace) -&gt; FastAPI:\n    \"\"\"Create the FastAPI application\n\n    See https://fastapi.tiangolo.com/tutorial/first-steps/ to learn how to\n    customize your FastAPI application\n    \"\"\"\n    if args.disable_fastapi_docs :\n        app = FastAPI(\n            openapi_url=None,\n            docs_url=None,\n            redoc_url=None,\n            title=f\"A REST API for vLLM\",\n            description=f\"A REST API for vLLM, production ready\",\n            lifespan=get_lifespan(async_engine_client, args=args),\n            version=utils.get_package_version()\n        )\n    else:\n        app = FastAPI(\n            title=f\"A REST API for vLLM\",\n            description=f\"A REST API for vLLM, production ready\",\n            lifespan=get_lifespan(async_engine_client, args=args),\n            version=utils.get_package_version()\n        )\n\n    # Add prometheus asgi middleware to route /metrics requests\n    mount_metrics(app)\n\n    # CORS middleware that allows all origins to avoid CORS problems\n    # see https://fastapi.tiangolo.com/tutorial/cors/#use-corsmiddleware\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n\n    # Add exception middleware\n    if args.explicit_errors:\n        app.add_middleware(ExceptionHandlerMiddleware)\n\n    #\n    app.include_router(main_routeur, prefix=args.api_endpoint_prefix)\n\n    app.root_path = args.root_path\n\n    app.state.enable_server_load_tracking = args.enable_server_load_tracking\n    app.state.server_load_metrics = 0\n\n    return app\n</code></pre>"},{"location":"reference/happy_vllm/launch/","title":"launch","text":""},{"location":"reference/happy_vllm/launch/#happy_vllm.launch.happy_vllm_build_async_engine_client","title":"<code>happy_vllm_build_async_engine_client(args)</code>","text":"<p>Replace vllm.entrypoints.openai.api_server.run_rpc_server by happy_vllm.run_rpc_server</p> Source code in <code>happy_vllm/launch.py</code> <pre><code>def happy_vllm_build_async_engine_client(args):\n    \"\"\"Replace vllm.entrypoints.openai.api_server.run_rpc_server by happy_vllm.run_rpc_server\n    \"\"\"\n    vllm_api_server.run_mp_engine  = run_mp_engine\n    return vllm_api_server.build_async_engine_client(args)\n</code></pre>"},{"location":"reference/happy_vllm/utils/","title":"utils","text":""},{"location":"reference/happy_vllm/utils/#happy_vllm.utils.get_input_ids","title":"<code>get_input_ids(tokenizer, *args, **kwargs)</code>","text":"<p>In case of a Mistral tokenizer, the input_ids are in an Encoding object</p> Source code in <code>happy_vllm/utils.py</code> <pre><code>def get_input_ids(tokenizer: AnyTokenizer, *args, **kwargs) -&gt; Union[List[int], List[List[int]]]:\n    \"In case of a Mistral tokenizer, the input_ids are in an Encoding object\"\n    input_ids_tmp = tokenizer(*args, **kwargs)\n    if isinstance(input_ids_tmp, Encoding):\n        input_ids = input_ids_tmp.input_ids\n    else:\n        input_ids = input_ids_tmp[\"input_ids\"]\n    return input_ids\n</code></pre>"},{"location":"reference/happy_vllm/utils/#happy_vllm.utils.get_package_version","title":"<code>get_package_version()</code>","text":"<p>Returns the current version of the package</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>version of the package</p> Source code in <code>happy_vllm/utils.py</code> <pre><code>def get_package_version() -&gt; str:\n    '''Returns the current version of the package\n\n    Returns:\n        str: version of the package\n    '''\n    version = importlib.metadata.version(\"happy_vllm\")\n    return version\n</code></pre>"},{"location":"reference/happy_vllm/utils/#happy_vllm.utils.get_vllm_version","title":"<code>get_vllm_version()</code>","text":"<p>Returns the installed version of vLLM </p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>version of vLLM</p> Source code in <code>happy_vllm/utils.py</code> <pre><code>def get_vllm_version() -&gt; str:\n    '''Returns the installed version of vLLM \n\n    Returns:\n        str: version of vLLM\n    '''\n    version = importlib.metadata.version(\"vllm\")\n    return version\n</code></pre>"},{"location":"reference/happy_vllm/utils/#happy_vllm.utils.proper_decode","title":"<code>proper_decode(tokenizer, token_ids)</code>","text":"<p>Gets the corresponding string from the token ids. We must use this technique in order to get the right string with their whitespace in the case of some tokenizers (e.g. Llama) deleting whitespace in front of the tokenized sentence</p> Source code in <code>happy_vllm/utils.py</code> <pre><code>def proper_decode(tokenizer, token_ids: Union[int, List[int]]) -&gt; str:\n    \"\"\"Gets the corresponding string from the token ids. We must use this technique in order to get the right string\n    with their whitespace in the case of some tokenizers (e.g. Llama) deleting whitespace in front of the tokenized sentence\n    \"\"\"\n    if isinstance(token_ids, int):\n        token_ids = [token_ids]\n    extra_token_id = proper_tokenization(tokenizer, 'c')[0]\n    token_ids = [extra_token_id] + token_ids\n    return tokenizer.decode(token_ids)[1:]\n</code></pre>"},{"location":"reference/happy_vllm/utils/#happy_vllm.utils.proper_tokenization","title":"<code>proper_tokenization(tokenizer, str_to_tokenize)</code>","text":"<p>Gets the token ids for a str. We must use this technique in order to get the right token ids with their whitespace in the case of some tokenizers (e.g. Llama) adding whitespace in front of the tokenized sentence</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A tokenizer</p> required <code>str_to_tokenize</code> <code>str) </code> <p>The string one wants to tokenize</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>The tuple containing the token ids for the input string</p> Source code in <code>happy_vllm/utils.py</code> <pre><code>def proper_tokenization(tokenizer, str_to_tokenize: str) -&gt; tuple:\n    \"\"\"Gets the token ids for a str. We must use this technique in order to get the right token ids\n    with their whitespace in the case of some tokenizers (e.g. Llama) adding whitespace in front of the tokenized sentence\n\n    Args:\n        tokenizer : A tokenizer\n        str_to_tokenize (str) : The string one wants to tokenize\n\n    Returns:\n        tuple : The tuple containing the token ids for the input string\n    \"\"\"\n    big_word = 'thisisareallybigwordisntit'\n    token_ids_big_word = get_input_ids(tokenizer, big_word, add_special_tokens=False)\n    # We concatenate the big word with the str in order for the str not to be at the beginning of the sentence\n    new_text = big_word + str_to_tokenize\n    token_ids_new_text = get_input_ids(tokenizer, new_text, add_special_tokens=False)\n    # We check that part of the str was not \"integrated\" in a token of the big_word\n    while token_ids_big_word != token_ids_new_text[:len(token_ids_big_word)]:\n        # If it has been \"integrated\", we take another big_word\n        big_word = big_word[:-1]\n        token_ids_big_word = get_input_ids(tokenizer, big_word, add_special_tokens=False)\n        # We concatenate the big word with the str in order for the str not to be at the beginning of the sentence\n        new_text = big_word + str_to_tokenize\n        token_ids_new_text = get_input_ids(tokenizer, new_text, add_special_tokens=False)\n    token_ids_str = tuple(token_ids_new_text[len(token_ids_big_word):])\n    return token_ids_str\n</code></pre>"},{"location":"reference/happy_vllm/utils_args/","title":"utils_args","text":""},{"location":"reference/happy_vllm/utils_args/#happy_vllm.utils_args.ApplicationSettings","title":"<code>ApplicationSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Application settings</p> <p>This class is used for settings management purpose, have a look at the pydantic documentation for more details : https://pydantic-docs.helpmanual.io/usage/settings/</p> <p>By default, it looks for environment variables (case insensitive) to set the settings if a variable is not found, it looks for a file name .env in your working directory where you can declare the values of the variables and finally it sets the values to the default ones one can define above</p> Source code in <code>happy_vllm/utils_args.py</code> <pre><code>class ApplicationSettings(BaseSettings):\n    \"\"\"Application settings\n\n    This class is used for settings management purpose, have a look at the pydantic\n    documentation for more details : https://pydantic-docs.helpmanual.io/usage/settings/\n\n    By default, it looks for environment variables (case insensitive) to set the settings\n    if a variable is not found, it looks for a file name .env in your working directory\n    where you can declare the values of the variables and finally it sets the values\n    to the default ones one can define above\n    \"\"\"\n    host : str = DEFAULT_HOST\n    port: int = DEFAULT_PORT\n    explicit_errors: bool = DEFAULT_EXPLICIT_ERRORS\n    allow_credentials: bool = DEFAULT_ALLOW_CREDENTIALS\n    allowed_origins: list = DEFAULT_ALLOWED_ORIGINS\n    allowed_methods: list = DEFAULT_ALLOWED_METHODS\n    allowed_headers: list = DEFAULT_ALLOWED_HEADERS\n    uvicorn_log_level: str = DEFAULT_UVICORN_LOG_LEVEL\n    ssl_keyfile: Optional[str] = DEFAULT_SSL_KEYFILE\n    ssl_certfile: Optional[str] = DEFAULT_SSL_CERTFILE\n    ssl_ca_certs: Optional[str] = DEFAULT_SSL_CA_CERTS\n    ssl_cert_reqs: int = DEFAULT_SSL_CERT_REQS\n    enable_ssl_refresh: bool = DEFAULT_ENABLE_SSL_REFRESH\n    root_path: Optional[str] = DEFAULT_ROOT_PATH\n    app_name: str = DEFAULT_APP_NAME\n    api_endpoint_prefix: str = DEFAULT_API_ENDPOINT_PREFIX\n    lora_modules: Optional[str] = DEFAULT_LORA_MODULES\n    chat_template : Optional[str] = DEFAULT_CHAT_TEMPLATE\n    chat_template_content_format: str = DEFAULT_CHAT_TEMPLATE_CONTENT_FORMAT\n    response_role: str = DEFAULT_RESPONSE_ROLE\n    with_launch_arguments: bool = DEFAULT_WITH_LAUNCH_ARGUMENTS\n    max_log_len: Optional[int] = DEFAULT_MAX_LOG_LEN\n    prompt_adapters: Optional[str] = DEFAULT_PROMPT_ADAPTERS\n    return_tokens_as_token_ids: bool = DEFAULT_RETURN_TOKENS_AS_TOKEN_IDS\n    disable_frontend_multiprocessing: bool = DEFAULT_DISABLE_FRONTEND_MULTIPROCESSING\n    enable_request_id_headers: bool = DEFAULT_ENABLE_REQUEST_ID_HEADERS\n    enable_auto_tool_choice: bool = DEFAULT_ENABLE_AUTO_TOOL_CHOICE\n    tool_call_parser: Optional[str] = DEFAULT_TOOL_CALL_PARSER\n    tool_parser_plugin: Optional[str] = DEFAULT_TOOL_PARSER_PLUGIN\n    disable_fastapi_docs : Optional[bool] = DEFAULT_DISABLE_FASTAPI_DOCS\n    enable_prompt_tokens_details : Optional[bool] = DEFAULT_ENABLE_PROMPT_TOKENS_DETAILS\n    enable_server_load_tracking: Optional[bool]= DEFAULT_ENABLE_SERVER_LOAD_TRACKING\n    disable_uvicorn_access_log: bool = DEFAULT_DISABLE_UVICORN_ACCESS_LOG\n\n\n    model_config = SettingsConfigDict(env_file=\".env\", extra='ignore', protected_namespaces=('settings', ))\n</code></pre>"},{"location":"reference/happy_vllm/utils_args/#happy_vllm.utils_args.get_model_settings","title":"<code>get_model_settings(parser)</code>","text":"<p>Gets the model settings. It corresponds to the variables added via AsyncEngineArgs.add_cli_args plus model-name and extra-information. First we use the parser to get the default values of vLLM for these variables. We instantiate a BaseSettings model with these values as default. They are possibly overwritten by environnement variables or those of a .env</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>FlexibleArgumentParser) </code> <p>The parser containing all the model variables with their default values from vLLM</p> required Source code in <code>happy_vllm/utils_args.py</code> <pre><code>def get_model_settings(parser: FlexibleArgumentParser) -&gt; BaseSettings:\n    \"\"\"Gets the model settings. It corresponds to the variables added via AsyncEngineArgs.add_cli_args plus model-name and extra-information.\n    First we use the parser to get the default values of vLLM for these variables. We instantiate a BaseSettings model\n    with these values as default. They are possibly overwritten by environnement variables or those of a .env\n\n    Args:\n        parser (FlexibleArgumentParser) : The parser containing all the model variables with their default values from vLLM \n    \"\"\"\n\n    default_args = parser.parse_args([])\n    # Define ModelSettings with the default values of the args (which will be replaced)\n    # by the environnement variables or those of the .env file\n    class ModelSettings(BaseSettings):\n        model: str = default_args.model\n        model_name: str = default_args.model_name\n        extra_information: Optional[str] = default_args.extra_information\n        served_model_name: Optional[Union[str, List[str]]] = None\n        tokenizer: Optional[str] = default_args.tokenizer\n        hf_config_path: Optional[str] = default_args.hf_config_path\n        task: TaskOption = default_args.task\n        skip_tokenizer_init: bool = False\n        tokenizer_mode: str = default_args.tokenizer_mode\n        trust_remote_code: bool = False\n\n        allowed_local_media_path: Optional[str] = default_args.allowed_local_media_path\n        download_dir: Optional[str] = default_args.download_dir\n        load_format: str = default_args.load_format\n        config_format: ConfigFormat = default_args.config_format\n        dtype: str = default_args.dtype\n        kv_cache_dtype: str = default_args.kv_cache_dtype\n        seed: Optional[int] = default_args.seed\n        max_model_len: Optional[int] = default_args.max_model_len\n        distributed_executor_backend: Optional[Union[str, ExecutorBase]] = default_args.distributed_executor_backend\n        pipeline_parallel_size: int = default_args.pipeline_parallel_size\n        tensor_parallel_size: int = default_args.tensor_parallel_size\n        enable_expert_parallel: bool = False\n        max_parallel_loading_workers: Optional[int] = default_args.max_parallel_loading_workers\n        block_size: Optional[int] = default_args.block_size\n        enable_prefix_caching: Optional[bool] = default_args.enable_prefix_caching\n        disable_sliding_window: bool = False\n        disable_cascade_attn: bool = False\n        swap_space: float = default_args.swap_space # GiB\n        cpu_offload_gb: float = default_args.cpu_offload_gb  # GiB\n        gpu_memory_utilization: float = default_args.gpu_memory_utilization\n        max_num_batched_tokens: Optional[int] = default_args.max_num_batched_tokens\n        max_num_partial_prefills: Optional[int] = default_args.max_num_partial_prefills\n        max_long_partial_prefills: Optional[int] = default_args.max_long_partial_prefills\n        long_prefill_token_threshold: Optional[int] = default_args.long_prefill_token_threshold\n        max_num_seqs: Optional[int] = default_args.max_num_seqs\n        disable_log_stats: bool = False\n        revision: Optional[str] = default_args.revision\n        code_revision: Optional[str] = default_args.code_revision\n        rope_scaling: Optional[Dict[str, Any]] = default_args.rope_scaling\n        rope_theta: Optional[float] = None\n        hf_overrides: Optional[HfOverrides] = default_args.hf_overrides\n        tokenizer_revision: Optional[str] = default_args.tokenizer_revision\n        quantization: Optional[str] = default_args.quantization\n        enforce_eager: Optional[bool] = default_args.enforce_eager\n        max_seq_len_to_capture: int = default_args.max_seq_len_to_capture\n        disable_custom_all_reduce: bool = False\n        enable_lora: bool = False\n        enable_lora_bias: bool = False\n        max_loras: int = default_args.max_loras\n        max_lora_rank: int = default_args.max_lora_rank\n        enable_prompt_adapter: bool = False\n        max_prompt_adapters: int = default_args.max_prompt_adapters\n        max_prompt_adapter_token: int = default_args.max_prompt_adapter_token\n        fully_sharded_loras: bool = False\n        lora_extra_vocab_size: int = default_args.lora_extra_vocab_size\n        long_lora_scaling_factors: Optional[Tuple[float]] = default_args.long_lora_scaling_factors\n        lora_dtype: Optional[Union[str, torch.dtype]] = default_args.lora_dtype\n        max_cpu_loras: Optional[int] = default_args.max_cpu_loras\n        device: str = default_args.device\n        num_scheduler_steps: int = default_args.num_scheduler_steps\n        multi_step_stream_outputs: bool = default_args.multi_step_stream_outputs\n        ray_workers_use_nsight: bool = False\n        num_gpu_blocks_override: Optional[int] = default_args.num_gpu_blocks_override\n        num_lookahead_slots: int = default_args.num_lookahead_slots\n        model_loader_extra_config: Optional[dict] = default_args.model_loader_extra_config\n        ignore_patterns:  Optional[Union[str, List[str]]] = default_args.ignore_patterns\n        preemption_mode: Optional[str] = default_args.preemption_mode\n        disable_log_requests: bool = False\n        engine_use_ray: bool = False\n        use_v2_block_manager: bool = default_args.use_v2_block_manager\n        max_logprobs: int = default_args.max_logprobs\n        tokenizer_pool_size: int = default_args.tokenizer_pool_size\n        tokenizer_pool_type: Union[str, BaseTokenizerGroup] = default_args.tokenizer_pool_type\n        tokenizer_pool_extra_config: Optional[Dict[str, Any]] = default_args.tokenizer_pool_extra_config\n        limit_mm_per_prompt: Optional[Mapping[str, int]] = default_args.limit_mm_per_prompt\n        mm_processor_kwargs: Optional[Dict[str, Any]] = default_args.mm_processor_kwargs\n        disable_mm_preprocessor_cache: bool = False\n        scheduling_policy: Literal[\"fcfs\", \"priority\"] = default_args.scheduling_policy\n        scheduler_cls: Union[str, Type[object]] = default_args.scheduler_cls\n        scheduler_delay_factor: float = default_args.scheduler_delay_factor\n        enable_chunked_prefill: Optional[bool] = default_args.enable_chunked_prefill\n        guided_decoding_backend: str = default_args.guided_decoding_backend\n        logits_processor_pattern: Optional[str] = default_args.logits_processor_pattern\n        speculative_config: Optional[Union[str, Dict[str, Any]]] = default_args.speculative_config\n        speculative_model: Optional[str] = default_args.speculative_model\n        speculative_model_quantization: Optional[str] = default_args.speculative_model_quantization\n        speculative_draft_tensor_parallel_size: Optional[int] = default_args.speculative_draft_tensor_parallel_size\n        num_speculative_tokens: Optional[int] = default_args.num_speculative_tokens\n        speculative_disable_mqa_scorer: Optional[bool] = default_args.speculative_disable_mqa_scorer\n        speculative_max_model_len: Optional[int] = default_args.speculative_max_model_len\n        speculative_disable_by_batch_size: Optional[int] = default_args.speculative_disable_by_batch_size\n        ngram_prompt_lookup_max: Optional[int] = default_args.ngram_prompt_lookup_max\n        ngram_prompt_lookup_min: Optional[int] = default_args.ngram_prompt_lookup_min\n        spec_decoding_acceptance_method: str = default_args.spec_decoding_acceptance_method\n        typical_acceptance_sampler_posterior_threshold: Optional[float] = default_args.typical_acceptance_sampler_posterior_threshold\n        typical_acceptance_sampler_posterior_alpha: Optional[float] = default_args.typical_acceptance_sampler_posterior_alpha\n        disable_logprobs_during_spec_decoding: Optional[bool] = default_args.disable_logprobs_during_spec_decoding\n        qlora_adapter_name_or_path: Optional[str] = default_args.qlora_adapter_name_or_path\n        show_hidden_metrics_for_version: Optional[str] = default_args.show_hidden_metrics_for_version\n        disable_async_output_proc: bool = False\n        override_neuron_config: Optional[Dict[str, Any]] = default_args.override_neuron_config\n        override_pooler_config: Optional[PoolerConfig] = default_args.override_pooler_config\n        compilation_config: Optional[CompilationConfig] = default_args.compilation_config\n        worker_cls: str = default_args.worker_cls\n        worker_extension_cls: str = default_args.worker_extension_cls\n        kv_transfer_config: Optional[KVTransferConfig] = default_args.kv_transfer_config\n        generation_config: Optional[str] = default_args.generation_config\n        override_generation_config: Optional[Dict[str, Any]] = default_args.override_generation_config\n        enable_sleep_mode: bool = False\n        model_impl: str = default_args.model_impl\n\n        calculate_kv_scales: Optional[bool] = default_args.calculate_kv_scales\n        additional_config: Optional[Dict[str, Any]] = default_args.additional_config\n        enable_reasoning: Optional[bool] = default_args.enable_reasoning\n        reasoning_parser: Optional[str] = default_args.reasoning_parser\n        use_tqdm_on_load: bool = default_args.use_tqdm_on_load\n        otlp_traces_endpoint: Optional[str] = default_args.otlp_traces_endpoint\n        collect_detailed_traces: Optional[str] = default_args.collect_detailed_traces\n\n        model_config = SettingsConfigDict(env_file=\".env\", extra='ignore', protected_namespaces=('settings', ))\n\n    model_settings = ModelSettings()\n    return model_settings\n</code></pre>"},{"location":"reference/happy_vllm/utils_args/#happy_vllm.utils_args.get_parser","title":"<code>get_parser()</code>","text":"<p>Gets the parser. The default values of all application variables (see ApplicationSettings) are properly set to the BaseSetting value defined via pydantic. The default values of all model variables (ie those added via AsyncEngineArgs.add_cli_args plus model-name) are not properly set via pydantic at this point.</p> <p>Returns:</p> Name Type Description <code>FlexibleArgumentParser</code> <code>FlexibleArgumentParser</code> <p>The argparse parser</p> Source code in <code>happy_vllm/utils_args.py</code> <pre><code>def get_parser() -&gt; FlexibleArgumentParser:\n    \"\"\"Gets the parser. The default values of all application variables (see ApplicationSettings) are properly\n    set to the BaseSetting value defined via pydantic. The default values of all model variables (ie those added\n    via AsyncEngineArgs.add_cli_args plus model-name) are not properly set via pydantic at this point.\n\n    Returns:\n        FlexibleArgumentParser : The argparse parser\n    \"\"\"\n    parser = FlexibleArgumentParser(description=\"REST API server for vLLM, production ready\")\n\n    application_settings = ApplicationSettings(_env_parse_none_str='None') # type: ignore\n\n    parser.add_argument(\"--host\",\n                        type=str,\n                        default=application_settings.host,\n                        help=\"host name\")\n    parser.add_argument(\"--port\",\n                        type=int,\n                        default=application_settings.port,\n                        help=\"port number\")\n    parser.add_argument(\"--model-name\",\n                        type=str,\n                        default=DEFAULT_MODEL_NAME,\n                        help=\"The name of the model given by the /info endpoint of the API\")\n    parser.add_argument(\"--extra-information\",\n                        type=str,\n                        default=DEFAULT_EXTRA_INFORMATION,\n                        help=\"The path to a json to add to the /info endpoint of the API\")\n    parser.add_argument(\"--app-name\",\n                        type=str,\n                        default=application_settings.app_name,\n                        help=\"The name of the application\")\n    parser.add_argument(\"--api-endpoint-prefix\",\n                        type=str,\n                        default=application_settings.api_endpoint_prefix,\n                        help=\"The prefix for the API endpoints\")\n    parser.add_argument(\"--explicit-errors\",\n                        default=application_settings.explicit_errors,\n                        action=BooleanOptionalAction,\n                        help=\"If True, the underlying python errors are sent back via the API\")\n    parser.add_argument('--allow-credentials',\n                        default=application_settings.allow_credentials,\n                        action=BooleanOptionalAction,\n                        help=\"allow credentials for CORS\")\n    parser.add_argument(\"--allowed-origins\",\n                        type=json.loads,\n                        default=application_settings.allowed_origins,\n                        help=\"CORS allowed origins\")\n    parser.add_argument(\"--allowed-methods\",\n                        type=json.loads,\n                        default=application_settings.allowed_methods,\n                        help=\"CORS allowed methods\")\n    parser.add_argument(\"--allowed-headers\",\n                        type=json.loads,\n                        default=application_settings.allowed_headers,\n                        help=\"CORS allowed headers\")\n    parser.add_argument(\"--uvicorn-log-level\",\n                        type=str,\n                        default=application_settings.uvicorn_log_level,\n                        choices=CHOICES_UVICORN_LOG_LEVEL,\n                        help=\"log level for uvicorn\")\n    parser.add_argument(\"--ssl-keyfile\",\n                        type=str,\n                        default=application_settings.ssl_keyfile,\n                        help=\"The file path to the SSL key file\")\n    parser.add_argument(\"--ssl-certfile\",\n                        type=str,\n                        default=application_settings.ssl_certfile,\n                        help=\"The file path to the SSL cert file\")\n    parser.add_argument(\"--ssl-ca-certs\",\n                        type=str,\n                        default=application_settings.ssl_ca_certs,\n                        help=\"The CA certificates file\")\n    parser.add_argument(\"--enable-ssl-refresh\",\n                        action=\"store_true\",\n                        default=application_settings.enable_ssl_refresh,\n                        help=\"Refresh SSL Context when SSL certificate files change\")\n    parser.add_argument(\"--ssl-cert-reqs\",\n                        type=int,\n                        default=application_settings.ssl_cert_reqs,\n                        help=\"Whether client certificate is required (see stdlib ssl module's)\")\n    parser.add_argument(\"--root-path\",\n                        type=str,\n                        default=application_settings.root_path,\n                        help=\"FastAPI root_path when app is behind a path based routing proxy\")\n    parser.add_argument(\"--lora-modules\",\n                        type=str,\n                        default=application_settings.lora_modules,\n                        nargs='+',\n                        action=LoRAParserAction,\n                        help=\"LoRA module configurations in the format name=path. \"\n                        \"Multiple modules can be specified.\")\n    parser.add_argument(\"--chat-template\",\n                        type=str,\n                        default=application_settings.chat_template,\n                        help=\"The file path to the chat template, \"\n                        \"or the template in single-line form \"\n                        \"for the specified model\")\n    parser.add_argument('--chat-template-content-format',\n                        type=str,\n                        default=application_settings.chat_template_content_format,\n                        choices=get_args(ChatTemplateContentFormatOption),\n                        help='The format to render message content within a chat template.'\n                        '\\n\\n'\n                        '* \"string\" will render the content as a string. '\n                        'Example: \"Hello World\"\\n'\n                        '* \"openai\" will render the content as a list of dictionaries, '\n                        'similar to OpenAI schema. '\n                        'Example: [{\"type\": \"text\", \"text\": \"Hello world!\"}]')\n    parser.add_argument(\"--response-role\",\n                        type=str,\n                        default=application_settings.response_role,\n                        help=\"The role name to return if \"\n                        \"`request.add_generation_prompt=true`.\")\n    parser.add_argument(\"--with-launch-arguments\",\n                        type=bool,\n                        default=application_settings.with_launch_arguments,\n                        help=\"Whether the route launch_arguments should display the launch arguments\")\n    parser.add_argument(\"--return-tokens-as-token-ids\",\n                        default=application_settings.return_tokens_as_token_ids,\n                        action=BooleanOptionalAction,\n                        help=\"When --max-logprobs is specified, represents single tokens as \"\n                        \"strings of the form 'token_id:{token_id}' so that tokens that \"\n                        \"are not JSON-encodable can be identified.\")\n    parser.add_argument('--max-log-len',\n                        type=int,\n                        default=application_settings.max_log_len,\n                        help='Max number of prompt characters or prompt '\n                        'ID numbers being printed in log.'\n                        '\\n\\nDefault: Unlimited')\n    parser.add_argument(\"--prompt-adapters\",\n                        type=nullable_str,\n                        default=application_settings.prompt_adapters,\n                        nargs='+',\n                        action=PromptAdapterParserAction,\n                        help=\"Prompt adapter configurations in the format name=path. \"\n                        \"Multiple adapters can be specified.\")\n    parser.add_argument(\"--disable-frontend-multiprocessing\",\n                        default=application_settings.disable_frontend_multiprocessing,\n                        action=BooleanOptionalAction,\n                        help=\"If specified, will run the OpenAI frontend server in the same \"\n                        \"process as the model serving engine.\")\n    parser.add_argument(\"--enable-request-id-headers\",\n                        default=application_settings.enable_request_id_headers,\n                        action=BooleanOptionalAction,\n                        help=\"If specified, API server will add X-Request-Id header to \"\n                        \"responses. Caution: this hurts performance at high QPS.\")\n    parser.add_argument(\"--enable-auto-tool-choice\",\n                        default=application_settings.enable_auto_tool_choice,\n                        action=BooleanOptionalAction,\n                        help=\n                        \"Enable auto tool choice for supported models. Use --tool-call-parser\"\n                        \"to specify which parser to use\")\n    valid_reasoning_parsers = ReasoningParserManager.reasoning_parsers.keys()\n    valid_tool_parsers = ToolParserManager.tool_parsers.keys()\n    parser.add_argument(\"--tool-call-parser\",\n                        type=str,\n                        metavar=\"{\" + \",\".join(valid_tool_parsers) + \"} or name registered in \"\n                        \"--tool-parser-plugin\",\n                        default=application_settings.tool_call_parser,\n                        help=\n                        \"Select the tool call parser depending on the model that you're using.\"\n                        \" This is used to parse the model-generated tool call into OpenAI API \"\n                        \"format. Required for --enable-auto-tool-choice.\")\n    parser.add_argument(\"--tool-parser-plugin\",\n                        type=str,\n                        default=application_settings.tool_parser_plugin,\n                        help=\n                        \"Special the tool parser plugin write to parse the model-generated tool\"\n                        \" into OpenAI API format, the name register in this plugin can be used \"\n                        \"in --tool-call-parser.\")\n    parser.add_argument(\"--disable-fastapi-docs\",\n                        action='store_true',\n                        default=application_settings.disable_fastapi_docs,\n                        help=\"Disable FastAPI's OpenAPI schema, Swagger UI, and ReDoc endpoint\")\n    parser.add_argument(\"--enable-prompt-tokens-details\",\n                        action='store_true',\n                        default=application_settings.enable_prompt_tokens_details,\n                        help=\"If set to True, enable prompt_tokens_details in usage.\")\n    parser.add_argument(\"--enable-server-load-tracking\",\n                        action='store_true',\n                        default=application_settings.enable_server_load_tracking,\n                        help=\"If set to True, enable tracking server_load_metrics in the app state.\")\n    parser.add_argument(\"--disable-uvicorn-access-log\",\n                        action=\"store_true\",\n                        help=\"Disable uvicorn access log.\")\n\n    parser = AsyncEngineArgs.add_cli_args(parser)\n    return parser\n</code></pre>"},{"location":"reference/happy_vllm/utils_args/#happy_vllm.utils_args.parse_args","title":"<code>parse_args()</code>","text":"<p>Parses the args. We want this priority : args from cli &gt; environnement variables &gt; .env variables. In order to do this, we get from pydantic BaseSetting the value from environnement variables or .env variables with the proper priority. Then we set the default value of the cli parser to those value so that if the cli args are used, they overwrite the default values and otherwise, the BaseSetting value is taken.</p> <p>Returns:</p> Type Description <code>Namespace</code> <p>NameSpace</p> Source code in <code>happy_vllm/utils_args.py</code> <pre><code>def parse_args() -&gt; Namespace:\n    \"\"\"Parses the args. We want this priority : args from cli &gt; environnement variables &gt; .env variables.\n    In order to do this, we get from pydantic BaseSetting the value from environnement variables or .env variables\n    with the proper priority. Then we set the default value of the cli parser to those value so that if the cli args\n    are used, they overwrite the default values and otherwise, the BaseSetting value is taken.\n\n    Returns:\n        NameSpace\n    \"\"\"\n    # Gets the parser\n    # The default value of the application variables are properly set\n    # Those of the model variables are not\n    parser = get_parser()\n    # Gets the default values of the model variables via pydantic\n    model_settings = get_model_settings(parser)\n    # Sets the default values of the model variables in the parser\n    parser.set_defaults(**model_settings.model_dump())\n    # Gets the args\n    args = parser.parse_args()\n    # Explicitly check for help flag for the providing help message to the entrypoint\n    if '-h' in sys.argv[1:] or '--help' in sys.argv[1:]:\n        parser.print_help()\n        sys.exit()\n    return args\n</code></pre>"},{"location":"reference/happy_vllm/core/","title":"core","text":"<p>Core package contains global configurations and utilities</p>"},{"location":"reference/happy_vllm/core/config/","title":"config","text":"<p>Config global settings</p> <p>This module handle global app configuration</p>"},{"location":"reference/happy_vllm/core/logtools/","title":"logtools","text":"<p>Logs utilities</p> <p>This module is used to define log pattern, add log filters, etc.</p>"},{"location":"reference/happy_vllm/core/resources/","title":"resources","text":"<p>Resources for the FastAPI application</p> <p>This module define resources that need to be instantiated at startup in a global variable resources that can be used in routes.</p> <p>This is the way your machine learning models can be loaded in memory at startup so they can handle requests.</p>"},{"location":"reference/happy_vllm/engine/","title":"engine","text":""},{"location":"reference/happy_vllm/engine/mp_engine/","title":"mp_engine","text":""},{"location":"reference/happy_vllm/middlewares/","title":"middlewares","text":""},{"location":"reference/happy_vllm/middlewares/exception/","title":"exception","text":""},{"location":"reference/happy_vllm/model/","title":"model","text":"<p>Model package contain model-related logic</p>"},{"location":"reference/happy_vllm/model/model_base/","title":"model_base","text":"<p>This module contains the base Model class</p>"},{"location":"reference/happy_vllm/model/model_base/#happy_vllm.model.model_base.Model","title":"<code>Model</code>","text":"<p>Parent model class.</p> Source code in <code>happy_vllm/model/model_base.py</code> <pre><code>class Model:\n    \"\"\"Parent model class.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        '''Init. model class'''\n        self._model = None\n        self._tokenizer = None\n        self._model_conf = None\n        self._model_explainer = None\n        self.openai_serving_chat = None\n        self.openai_serving_embedding = None\n        self.openai_serving_completion = None\n        self.openai_serving_tokenization = None\n        self._loaded = False\n        self.app_name = kwargs.get('app_name', \"happy_vllm\")\n        self.extra_information = {}\n\n    def is_model_loaded(self):\n        \"\"\"return the state of the model\"\"\"\n        return self._loaded\n\n    async def loading(self, async_engine_client: MQLLMEngineClient, args: Namespace, **kwargs):\n        \"\"\"load the model\"\"\"\n        await self._load_model(async_engine_client, args, **kwargs)\n        self._loaded = True\n        if args.extra_information:\n            with open(args.extra_information, 'r') as json_file:\n                self.extra_information = json.load(json_file)\n        if args.with_launch_arguments:\n            self.launch_arguments = vars(args)\n        else:\n            self.launch_arguments = {}\n\n    async def _load_model(self, async_engine_client: MQLLMEngineClient, args: Namespace, **kwargs) -&gt; None:\n        \"\"\"Load a model from a file\n\n        Returns:\n            Tuple[Any, dict]: A tuple containing the model and a dict of metadata about it.\n        \"\"\"\n\n        self._model_conf = {'model_name': args.model_name}\n\n        logger.info(f\"Loading the model from {args.model}\")\n        if args.model_name != \"TEST MODEL\":\n            self._model = async_engine_client\n            model_config = await self._model.get_model_config()\n            # Define the tokenizer differently if we have an AsyncLLMEngine\n            if isinstance(self._model, AsyncLLMEngine):\n                tokenizer_tmp = self._model.engine.tokenizer\n            else:\n                tokenizer_tmp = self._model.tokenizer\n            if isinstance(tokenizer_tmp, TokenizerGroup): # type: ignore\n                self._tokenizer = tokenizer_tmp.tokenizer # type: ignore\n            else:\n                self._tokenizer = tokenizer_tmp # type: ignore\n            self.max_model_len = model_config.max_model_len # type: ignore\n            # To take into account Mistral tokenizers\n            try:\n                self.original_truncation_side = self._tokenizer.truncation_side # type: ignore\n            except:\n                self.original_truncation_side = \"left\"\n            if args.disable_log_requests:\n                request_logger = None\n            else:\n                request_logger = RequestLogger(max_log_len=args.max_log_len)\n            served_model_names = [args.model_name]\n            base_model_paths = [\n                BaseModelPath(name=name, model_path=args.model)\n                for name in served_model_names\n            ]\n            resolved_chat_template = load_chat_template(args.chat_template)\n            logger.info(\"Using supplied chat template:\\n%s\", resolved_chat_template)\n            self.openai_serving_models = OpenAIServingModels(\n                                            engine_client=cast(AsyncLLMEngine,self._model),\n                                            model_config=model_config,\n                                            base_model_paths=base_model_paths,\n                                            lora_modules=args.lora_modules,\n                                            prompt_adapters=args.prompt_adapters,\n                                        )\n            self.task = model_config.task\n            if model_config.task == \"generate\":\n                self.openai_serving_chat = OpenAIServingChat(cast(AsyncLLMEngine,self._model), model_config,\n                                                            self.openai_serving_models,\n                                                            args.response_role,\n                                                            request_logger=request_logger,\n                                                            chat_template=resolved_chat_template,\n                                                            chat_template_content_format=args.chat_template_content_format,\n                                                            return_tokens_as_token_ids=args.return_tokens_as_token_ids,\n                                                            enable_auto_tools=args.enable_auto_tool_choice,\n                                                            tool_parser=args.tool_call_parser,\n                                                            enable_reasoning=args.enable_reasoning,\n                                                            reasoning_parser=args.reasoning_parser,\n                                                            enable_prompt_tokens_details=args.enable_prompt_tokens_details)\n                self.openai_serving_completion = OpenAIServingCompletion(cast(AsyncLLMEngine,self._model), model_config, \n                                                                        self.openai_serving_models, \n                                                                        request_logger=request_logger,\n                                                                        return_tokens_as_token_ids=args.return_tokens_as_token_ids)\n            if model_config.task == \"embed\":\n                self.openai_serving_embedding = OpenAIServingEmbedding(cast(AsyncLLMEngine,self._model),\n                                                                        model_config,\n                                                                        self.openai_serving_models,\n                                                                        request_logger=request_logger,\n                                                                        chat_template=resolved_chat_template,\n                                                                        chat_template_content_format=args.chat_template_content_format)\n            if model_config.task == \"transcription\":\n                self.openai_serving_transcription = OpenAIServingTranscription(cast(AsyncLLMEngine,self._model),\n                                                                                model_config,\n                                                                                self.openai_serving_models,\n                                                                                request_logger=request_logger)\n            self.openai_serving_tokenization = OpenAIServingTokenization(cast(AsyncLLMEngine,self._model), model_config, \n                                                                        self.openai_serving_models,\n                                                                        request_logger=request_logger,\n                                                                        chat_template=resolved_chat_template,\n                                                                        chat_template_content_format=args.chat_template_content_format)\n\n        # For test purpose\n        else:\n            self.max_model_len = 2048\n            self.original_truncation_side = 'right'\n            self._tokenizer = AutoTokenizer.from_pretrained(utils.TEST_TOKENIZER_NAME,\n                                                     cache_dir=os.environ[\"TEST_MODELS_DIR\"], truncation_side=self.original_truncation_side)\n            self._model = MockModel(self._tokenizer, self.app_name)\n            self.openai_serving_tokenization = MockOpenAIServingTokenization(self._tokenizer)\n        logger.info(f\"Model loaded\")\n\n    def tokenize(self, text: str) -&gt; List[int]:\n        \"\"\"Tokenizes a text\n\n        Args:\n            text (str) : The text to tokenize\n\n        Returns:\n            list : The list of token ids\n        \"\"\"\n        return list(utils.proper_tokenization(self._tokenizer, text))\n\n    def split_text(self, text: str, num_tokens_in_chunk: int = 200, separators: Union[list, None] = None) -&gt; List[str]:\n        '''Splits a text in small texts containing at least num_tokens_in_chunk tokens and ending by a separator. note that the `separators` \n        used are the tokenization of the strings and not the strings themselves (which explains why we must for example \n        specify ' .' and '.' as two separate separators) \n\n        Args:\n            text (str) : The text to split\n\n        Kwargs:\n            num_tokens_in_chunk (int) : The minimal number of tokens in the chunk\n            separators (list) : The separators marking the end of a sentence\n\n        Returns:\n            A list of strings each string containing at least num_tokens_in_chunk tokens and ending by a separator\n        '''\n        if separators is None:\n            separators = [\".\", \"!\", \"?\", \"|\", \" .\", \" !\", \" ?\", \" |\"]\n        separators_tokens_ids = set()\n        for separator in separators:\n            separators_tokens_ids.add(utils.proper_tokenization(self._tokenizer, separator))\n        tokens = list(utils.proper_tokenization(self._tokenizer, text))\n        indices_separators = []\n        for separator_tokens_ids in separators_tokens_ids:\n            indices_separators += find_indices_sub_list_in_list(tokens, list(separator_tokens_ids))\n        indices_separators.sort()\n\n        chunks = []\n        index_beginning_chunk = 0\n        current_used_separator = 0\n        while current_used_separator &lt; len(indices_separators):\n            index_current_used_separator = indices_separators[current_used_separator]\n            if index_current_used_separator +1 - index_beginning_chunk &gt;= num_tokens_in_chunk:\n                chunks.append(tokens[index_beginning_chunk:index_current_used_separator + 1])\n                index_beginning_chunk = index_current_used_separator + 1\n            current_used_separator += 1\n        chunks.append(tokens[index_beginning_chunk:])\n        chunks_decoded = [utils.proper_decode(self._tokenizer, chunk) for chunk in chunks]\n        chunks_decoded = [element for element in chunks_decoded if element!= \"\"]\n        return chunks_decoded\n\n    def extract_text_outside_truncation(self, text: str, truncation_side: Union[str, None] = None, max_length: Union[int, None] = None) -&gt; str:\n        \"\"\"Extracts the part of the prompt not kept after truncation, which will not be infered by the model.\n        First, we tokenize the prompt while applying truncation.\n        We obtain a list of sequences of token ids padded, which are outside the truncation.\n        Then we decode this list of tensors of token IDs containing special tokens to a string.\n\n        Args:\n            text (str) : The text we want to parse\n            truncation_side (str) : The side of the truncation\n            max_length (int) : The length above which the text will be truncated\n\n        Returns:\n            The part of the text which will be dropped by the truncation (str)\n        \"\"\"\n        if max_length is None:\n            max_length = self.max_model_len\n        if truncation_side is None:\n            truncation_side = self.original_truncation_side\n        self._tokenizer.truncation_side = truncation_side\n        list_tokens = self._tokenizer(text, truncation=True, add_special_tokens=False, max_length=max_length, return_overflowing_tokens=True)['input_ids']\n        if len(list_tokens) &lt;= 1:\n            return ''\n        not_truncated = list_tokens[0]\n        truncated_tmp = list_tokens[1:]\n        if self._tokenizer.truncation_side == 'left':\n            truncated_tmp.reverse()\n        truncated = []\n        for truncated_tokens in truncated_tmp:\n            truncated += truncated_tokens\n        truncated_str = self._tokenizer.decode(truncated)\n        self._tokenizer.truncation_side = self.original_truncation_side\n        return truncated_str\n</code></pre>"},{"location":"reference/happy_vllm/model/model_base/#happy_vllm.model.model_base.Model.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Init. model class</p> Source code in <code>happy_vllm/model/model_base.py</code> <pre><code>def __init__(self, **kwargs):\n    '''Init. model class'''\n    self._model = None\n    self._tokenizer = None\n    self._model_conf = None\n    self._model_explainer = None\n    self.openai_serving_chat = None\n    self.openai_serving_embedding = None\n    self.openai_serving_completion = None\n    self.openai_serving_tokenization = None\n    self._loaded = False\n    self.app_name = kwargs.get('app_name', \"happy_vllm\")\n    self.extra_information = {}\n</code></pre>"},{"location":"reference/happy_vllm/model/model_base/#happy_vllm.model.model_base.Model.extract_text_outside_truncation","title":"<code>extract_text_outside_truncation(text, truncation_side=None, max_length=None)</code>","text":"<p>Extracts the part of the prompt not kept after truncation, which will not be infered by the model. First, we tokenize the prompt while applying truncation. We obtain a list of sequences of token ids padded, which are outside the truncation. Then we decode this list of tensors of token IDs containing special tokens to a string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str) </code> <p>The text we want to parse</p> required <code>truncation_side</code> <code>str) </code> <p>The side of the truncation</p> <code>None</code> <code>max_length</code> <code>int) </code> <p>The length above which the text will be truncated</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The part of the text which will be dropped by the truncation (str)</p> Source code in <code>happy_vllm/model/model_base.py</code> <pre><code>def extract_text_outside_truncation(self, text: str, truncation_side: Union[str, None] = None, max_length: Union[int, None] = None) -&gt; str:\n    \"\"\"Extracts the part of the prompt not kept after truncation, which will not be infered by the model.\n    First, we tokenize the prompt while applying truncation.\n    We obtain a list of sequences of token ids padded, which are outside the truncation.\n    Then we decode this list of tensors of token IDs containing special tokens to a string.\n\n    Args:\n        text (str) : The text we want to parse\n        truncation_side (str) : The side of the truncation\n        max_length (int) : The length above which the text will be truncated\n\n    Returns:\n        The part of the text which will be dropped by the truncation (str)\n    \"\"\"\n    if max_length is None:\n        max_length = self.max_model_len\n    if truncation_side is None:\n        truncation_side = self.original_truncation_side\n    self._tokenizer.truncation_side = truncation_side\n    list_tokens = self._tokenizer(text, truncation=True, add_special_tokens=False, max_length=max_length, return_overflowing_tokens=True)['input_ids']\n    if len(list_tokens) &lt;= 1:\n        return ''\n    not_truncated = list_tokens[0]\n    truncated_tmp = list_tokens[1:]\n    if self._tokenizer.truncation_side == 'left':\n        truncated_tmp.reverse()\n    truncated = []\n    for truncated_tokens in truncated_tmp:\n        truncated += truncated_tokens\n    truncated_str = self._tokenizer.decode(truncated)\n    self._tokenizer.truncation_side = self.original_truncation_side\n    return truncated_str\n</code></pre>"},{"location":"reference/happy_vllm/model/model_base/#happy_vllm.model.model_base.Model.is_model_loaded","title":"<code>is_model_loaded()</code>","text":"<p>return the state of the model</p> Source code in <code>happy_vllm/model/model_base.py</code> <pre><code>def is_model_loaded(self):\n    \"\"\"return the state of the model\"\"\"\n    return self._loaded\n</code></pre>"},{"location":"reference/happy_vllm/model/model_base/#happy_vllm.model.model_base.Model.loading","title":"<code>loading(async_engine_client, args, **kwargs)</code>  <code>async</code>","text":"<p>load the model</p> Source code in <code>happy_vllm/model/model_base.py</code> <pre><code>async def loading(self, async_engine_client: MQLLMEngineClient, args: Namespace, **kwargs):\n    \"\"\"load the model\"\"\"\n    await self._load_model(async_engine_client, args, **kwargs)\n    self._loaded = True\n    if args.extra_information:\n        with open(args.extra_information, 'r') as json_file:\n            self.extra_information = json.load(json_file)\n    if args.with_launch_arguments:\n        self.launch_arguments = vars(args)\n    else:\n        self.launch_arguments = {}\n</code></pre>"},{"location":"reference/happy_vllm/model/model_base/#happy_vllm.model.model_base.Model.split_text","title":"<code>split_text(text, num_tokens_in_chunk=200, separators=None)</code>","text":"<p>Splits a text in small texts containing at least num_tokens_in_chunk tokens and ending by a separator. note that the <code>separators</code>  used are the tokenization of the strings and not the strings themselves (which explains why we must for example  specify ' .' and '.' as two separate separators) </p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str) </code> <p>The text to split</p> required Kwargs <p>num_tokens_in_chunk (int) : The minimal number of tokens in the chunk separators (list) : The separators marking the end of a sentence</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of strings each string containing at least num_tokens_in_chunk tokens and ending by a separator</p> Source code in <code>happy_vllm/model/model_base.py</code> <pre><code>def split_text(self, text: str, num_tokens_in_chunk: int = 200, separators: Union[list, None] = None) -&gt; List[str]:\n    '''Splits a text in small texts containing at least num_tokens_in_chunk tokens and ending by a separator. note that the `separators` \n    used are the tokenization of the strings and not the strings themselves (which explains why we must for example \n    specify ' .' and '.' as two separate separators) \n\n    Args:\n        text (str) : The text to split\n\n    Kwargs:\n        num_tokens_in_chunk (int) : The minimal number of tokens in the chunk\n        separators (list) : The separators marking the end of a sentence\n\n    Returns:\n        A list of strings each string containing at least num_tokens_in_chunk tokens and ending by a separator\n    '''\n    if separators is None:\n        separators = [\".\", \"!\", \"?\", \"|\", \" .\", \" !\", \" ?\", \" |\"]\n    separators_tokens_ids = set()\n    for separator in separators:\n        separators_tokens_ids.add(utils.proper_tokenization(self._tokenizer, separator))\n    tokens = list(utils.proper_tokenization(self._tokenizer, text))\n    indices_separators = []\n    for separator_tokens_ids in separators_tokens_ids:\n        indices_separators += find_indices_sub_list_in_list(tokens, list(separator_tokens_ids))\n    indices_separators.sort()\n\n    chunks = []\n    index_beginning_chunk = 0\n    current_used_separator = 0\n    while current_used_separator &lt; len(indices_separators):\n        index_current_used_separator = indices_separators[current_used_separator]\n        if index_current_used_separator +1 - index_beginning_chunk &gt;= num_tokens_in_chunk:\n            chunks.append(tokens[index_beginning_chunk:index_current_used_separator + 1])\n            index_beginning_chunk = index_current_used_separator + 1\n        current_used_separator += 1\n    chunks.append(tokens[index_beginning_chunk:])\n    chunks_decoded = [utils.proper_decode(self._tokenizer, chunk) for chunk in chunks]\n    chunks_decoded = [element for element in chunks_decoded if element!= \"\"]\n    return chunks_decoded\n</code></pre>"},{"location":"reference/happy_vllm/model/model_base/#happy_vllm.model.model_base.Model.tokenize","title":"<code>tokenize(text)</code>","text":"<p>Tokenizes a text</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str) </code> <p>The text to tokenize</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[int]</code> <p>The list of token ids</p> Source code in <code>happy_vllm/model/model_base.py</code> <pre><code>def tokenize(self, text: str) -&gt; List[int]:\n    \"\"\"Tokenizes a text\n\n    Args:\n        text (str) : The text to tokenize\n\n    Returns:\n        list : The list of token ids\n    \"\"\"\n    return list(utils.proper_tokenization(self._tokenizer, text))\n</code></pre>"},{"location":"reference/happy_vllm/model/model_base/#happy_vllm.model.model_base.find_indices_sub_list_in_list","title":"<code>find_indices_sub_list_in_list(big_list, sub_list)</code>","text":"<p>Find the indices of the presence of a sub list in a bigger list. For example if big_list = [3, 4, 1, 2, 3, 4, 5, 6, 3, 4] and sub_list = [3, 4], the result will be [1, 5, 9]</p> <p>Parameters:</p> Name Type Description Default <code>big_list</code> <code>list) </code> <p>The list in which we want to find the sub_list</p> required <code>sub_list</code> <code>list</code> <p>The list we want the indices of in the big_list</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>The list of indices of where the sub_list is in the big_list</p> Source code in <code>happy_vllm/model/model_base.py</code> <pre><code>def find_indices_sub_list_in_list(big_list: list, sub_list: list) -&gt; list:\n    \"\"\"Find the indices of the presence of a sub list in a bigger list. For example\n    if big_list = [3, 4, 1, 2, 3, 4, 5, 6, 3, 4] and sub_list = [3, 4],\n    the result will be [1, 5, 9]\n\n    Args:\n        big_list (list) : The list in which we want to find the sub_list\n        sub_list (list): The list we want the indices of in the big_list\n\n    Returns:\n        list : The list of indices of where the sub_list is in the big_list \n    \"\"\"\n    len_sub_list = len(sub_list)\n    indices = []\n    for index in range(len(big_list)):\n        if big_list[index - len_sub_list + 1: index + 1] == sub_list:\n            indices.append(index)\n    return indices\n</code></pre>"},{"location":"reference/happy_vllm/routers/","title":"routers","text":"<p>Main router of the REST API</p>"},{"location":"reference/happy_vllm/routers/functional/","title":"functional","text":""},{"location":"reference/happy_vllm/routers/functional/#happy_vllm.routers.functional.check_generator","title":"<code>check_generator(generator)</code>","text":"<p>Parses the LLM response to check if prompt_logprobs and fix '-inf' value</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code> (Union[ChatCompletionResponse, CompletionResponse]</code> <p>The request to verify</p> required <p>Returns:     None</p> Source code in <code>happy_vllm/routers/functional.py</code> <pre><code>def check_generator(generator: Union[\n    vllm_protocol.ChatCompletionResponse, \n    vllm_protocol.CompletionResponse\n    ]) -&gt; None:\n    \"\"\"Parses the LLM response to check if prompt_logprobs and fix '-inf' value\n\n    Args:\n        request  (Union[ChatCompletionResponse, CompletionResponse]): The request to verify\n    Returns:\n        None\n    \"\"\"\n    if hasattr(generator, \"prompt_logprobs\"):\n        if generator.prompt_logprobs:\n            for logprob_dict in generator.prompt_logprobs:\n                    if logprob_dict:\n                        for logprob_values in logprob_dict.values():\n                            if logprob_values.logprob == float('-inf'):\n                                logprob_values.logprob = -9999.0\n</code></pre>"},{"location":"reference/happy_vllm/routers/functional/#happy_vllm.routers.functional.create_chat_completion","title":"<code>create_chat_completion(request, raw_request)</code>  <code>async</code>","text":"<p>Open AI compatible chat completion. See https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html for more details</p> Source code in <code>happy_vllm/routers/functional.py</code> <pre><code>@router.post(\n    \"/v1/chat/completions\", \n    response_model=Union[\n        vllm_protocol.ErrorResponse, \n        functional_schema.HappyvllmChatCompletionResponse\n    ], \n    dependencies=[Depends(validate_json_request)]\n)\n@with_cancellation\n@load_aware_call\nasync def create_chat_completion(\n    request: Annotated[vllm_protocol.ChatCompletionRequest, \n    Body(openapi_examples=request_openapi_examples[\"chat_completions\"])],\n    raw_request: Request\n):\n    \"\"\"Open AI compatible chat completion. See https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html for more details\n    \"\"\"\n    model: Model = RESOURCES[RESOURCE_MODEL]\n    handler = model.openai_serving_chat\n    if handler is None:\n        raise HTTPException(\n            status_code=400, \n            detail=f\"The model does not support Chat Completions API\"\n        )\n    verify_request(request)\n    generator = await handler.create_chat_completion(\n        request, raw_request)\n    if isinstance(generator, vllm_protocol.ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    check_generator(generator)\n    if request.stream:\n        return StreamingResponse(content=generator, # type: ignore\n                                 media_type=\"text/event-stream\")\n    else:\n        return JSONResponse(content=generator.model_dump()) # type: ignore\n</code></pre>"},{"location":"reference/happy_vllm/routers/functional/#happy_vllm.routers.functional.create_completion","title":"<code>create_completion(request, raw_request)</code>  <code>async</code>","text":"<p>Open AI compatible completion. See https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html for more details</p> Source code in <code>happy_vllm/routers/functional.py</code> <pre><code>@router.post(\n    \"/v1/completions\", \n    response_model=Union[\n        vllm_protocol.ErrorResponse, \n        functional_schema.HappyvllmCompletionResponse\n    ], \n    dependencies=[Depends(validate_json_request)]\n)\n@with_cancellation\n@load_aware_call\nasync def create_completion(request: Annotated[vllm_protocol.CompletionRequest, Body(openapi_examples=request_openapi_examples[\"completions\"])],\n                            raw_request: Request):\n    \"\"\"Open AI compatible completion. See https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html for more details\n    \"\"\"\n    model: Model = RESOURCES[RESOURCE_MODEL]\n    handler = model.openai_serving_completion\n    if handler is None:\n        return base(raw_request, model).create_error_response(\n            message=\"The model does not support Completions API\")\n    verify_request(request)\n    generator = await handler.create_completion(\n        request, raw_request)\n    if isinstance(generator, vllm_protocol.ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    check_generator(generator)\n    if request.stream:\n        return StreamingResponse(content=generator,\n                                 media_type=\"text/event-stream\")\n    else:\n        return JSONResponse(content=generator.model_dump())\n</code></pre>"},{"location":"reference/happy_vllm/routers/functional/#happy_vllm.routers.functional.decode","title":"<code>decode(request, request_type=None)</code>  <code>async</code>","text":"<p>Decodes token ids</p> <p>The request should be a JSON object with the following fields: - token_ids: The ids of the tokens - with_tokens_str : If the result should also include a list of str - vanilla (optional) : Whether we want the vanilla version of the tokenizers</p> Source code in <code>happy_vllm/routers/functional.py</code> <pre><code>@router.post(\"/v1/decode\", response_model=functional_schema.ResponseDecode)\nasync def decode(request: Request,\n    request_type: Annotated[\n        functional_schema.RequestDecode,\n        Body(openapi_examples=request_openapi_examples[\"decode\"])] = None\n    ) -&gt; Response:\n    \"\"\"Decodes token ids\n\n    The request should be a JSON object with the following fields:\n    - token_ids: The ids of the tokens\n    - with_tokens_str : If the result should also include a list of str\n    - vanilla (optional) : Whether we want the vanilla version of the tokenizers\n    \"\"\"\n    model: Model = RESOURCES[RESOURCE_MODEL]\n    request_dict = await request.json()\n    token_ids = request_dict.pop(\"token_ids\")\n    with_tokens_str = request_dict.get(\"with_tokens_str\", False)\n    vanilla = request_dict.get(\"vanilla\", True)\n\n    if vanilla:\n        decoded_string = model._tokenizer.decode(token_ids)\n        if with_tokens_str:\n            tokens_str = model._tokenizer.convert_ids_to_tokens(token_ids)\n    else:\n        decoded_string = utils.proper_decode(model._tokenizer, token_ids)\n        if with_tokens_str:\n            tokens_str = [utils.proper_decode(model._tokenizer, token_id) for token_id in token_ids]\n\n\n    ret = {\"decoded_string\": decoded_string}\n    if with_tokens_str:\n        ret[ \"tokens_str\"] = tokens_str\n    return JSONResponse(ret)\n</code></pre>"},{"location":"reference/happy_vllm/routers/functional/#happy_vllm.routers.functional.decode_v2","title":"<code>decode_v2(request, raw_request)</code>  <code>async</code>","text":"<p>Decodes token ids</p> <p>The request should be a JSON object with the following fields: - tokens: The ids of the tokens - model : ID of the model to use</p> Source code in <code>happy_vllm/routers/functional.py</code> <pre><code>@router.post(\"/v2/decode\", response_model=vllm_protocol.DetokenizeResponse, dependencies=[Depends(validate_json_request)])\n@with_cancellation\nasync def decode_v2(request :Annotated[\n        vllm_protocol.DetokenizeRequest,\n        Body(openapi_examples=request_openapi_examples[\"vllm_decode\"])],\n        raw_request: Request\n    ):\n    \"\"\"Decodes token ids\n\n    The request should be a JSON object with the following fields:\n    - tokens: The ids of the tokens\n    - model : ID of the model to use\n    \"\"\"\n    model: Model = RESOURCES[RESOURCE_MODEL]\n    generator = await model.openai_serving_tokenization.create_detokenize(request, raw_request)\n    if isinstance(generator, vllm_protocol.ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    else:\n        if not isinstance(generator, vllm_protocol.DetokenizeResponse):\n            raise TypeError(\"Expected generator to be an instance of vllm_protocol.DetokenizeResponse\")\n        return JSONResponse(content=generator.model_dump())\n</code></pre>"},{"location":"reference/happy_vllm/routers/functional/#happy_vllm.routers.functional.metadata_text","title":"<code>metadata_text(request, request_type=None)</code>  <code>async</code>","text":"<p>Gives meta data on a text</p> <p>The request should be a JSON object with the following fields: - text: The text to parse - truncation_side (optional): The truncation side of the tokenizer - max_length (optional) : The max length before truncation</p> <p>The default values for truncation_side and max_length are those of the underlying model</p> Source code in <code>happy_vllm/routers/functional.py</code> <pre><code>@router.post(\"/v1/metadata_text\", response_model=functional_schema.ResponseMetadata)\nasync def metadata_text(request: Request,\n    request_type: Annotated[\n        functional_schema.RequestMetadata,\n        Body(openapi_examples=request_openapi_examples[\"metadata_text\"])] = None):\n    \"\"\"Gives meta data on a text\n\n    The request should be a JSON object with the following fields:\n    - text: The text to parse\n    - truncation_side (optional): The truncation side of the tokenizer\n    - max_length (optional) : The max length before truncation\n\n    The default values for truncation_side and max_length are those of the underlying model\n    \"\"\"\n    model: Model = RESOURCES[RESOURCE_MODEL]\n\n    request_dict = await request.json()\n\n    tokens_ids = model.tokenize(request_dict['text'])\n    truncated_text = model.extract_text_outside_truncation(**request_dict)\n    ret = {\"tokens_nb\": len(tokens_ids), \"truncated_text\": truncated_text}\n\n    return JSONResponse(ret)\n</code></pre>"},{"location":"reference/happy_vllm/routers/functional/#happy_vllm.routers.functional.split_text","title":"<code>split_text(request, request_type=None)</code>  <code>async</code>","text":"<p>Splits a text with a minimal number of token in each chunk. Each chunk is delimited by a separator</p> <p>The request should be a JSON object with the following fields: - text: The text to split - num_tokens_in_chunk (optional): The minimal number of tokens we want in each chunk - separators (optional) : The allowed separators between the chunks</p> Source code in <code>happy_vllm/routers/functional.py</code> <pre><code>@router.post(\"/v1/split_text\", response_model=functional_schema.ResponseSplitText)\nasync def split_text(request: Request,\n    request_type: Annotated[\n        functional_schema.RequestSplitText,\n        Body(openapi_examples=request_openapi_examples[\"split_text\"])] = None\n    ):\n    \"\"\"Splits a text with a minimal number of token in each chunk. Each chunk is delimited by a separator\n\n    The request should be a JSON object with the following fields:\n    - text: The text to split\n    - num_tokens_in_chunk (optional): The minimal number of tokens we want in each chunk\n    - separators (optional) : The allowed separators between the chunks\n    \"\"\"\n    model: Model = RESOURCES[RESOURCE_MODEL]\n\n    request_dict = await request.json()\n    split_text = model.split_text(**request_dict)\n    response = {\"split_text\": split_text}\n\n    return JSONResponse(response)\n</code></pre>"},{"location":"reference/happy_vllm/routers/functional/#happy_vllm.routers.functional.tokenizer","title":"<code>tokenizer(request, request_type=None)</code>  <code>async</code>","text":"<p>Tokenizes a text</p> <p>The request should be a JSON object with the following fields: - text: The text to tokenize - with_tokens_str (optional): Whether we want the tokens strings in the output - vanilla (optional) : Whether we want the vanilla version of the tokenizers</p> Source code in <code>happy_vllm/routers/functional.py</code> <pre><code>@router.post(\"/v1/tokenizer\", response_model=functional_schema.ResponseTokenizer)\nasync def tokenizer(request: Request,\n    request_type: Annotated[\n        functional_schema.RequestTokenizer,\n        Body(openapi_examples=request_openapi_examples[\"tokenizer\"])] = None\n    ) -&gt; Response:\n    \"\"\"Tokenizes a text\n\n    The request should be a JSON object with the following fields:\n    - text: The text to tokenize\n    - with_tokens_str (optional): Whether we want the tokens strings in the output\n    - vanilla (optional) : Whether we want the vanilla version of the tokenizers\n    \"\"\"\n    model: Model = RESOURCES[RESOURCE_MODEL]\n    request_dict = await request.json()\n    text = request_dict.pop(\"text\")\n    vanilla = request_dict.get(\"vanilla\", True)\n    with_tokens_str = request_dict.get('with_tokens_str', False)\n\n    if vanilla:\n        tokens_ids = utils.get_input_ids(model._tokenizer, text)\n        if with_tokens_str:\n            tokens_str = model._tokenizer.convert_ids_to_tokens(tokens_ids)\n    else:\n        tokens_ids = model.tokenize(text)\n        if with_tokens_str:\n            tokens_str = [utils.proper_decode(model._tokenizer, token_id) for token_id in tokens_ids]\n\n\n    ret = {\"tokens_ids\": tokens_ids, \"tokens_nb\": len(tokens_ids)}\n    if with_tokens_str:\n        ret['tokens_str'] = tokens_str\n    return JSONResponse(ret)\n</code></pre>"},{"location":"reference/happy_vllm/routers/functional/#happy_vllm.routers.functional.tokenizer_v2","title":"<code>tokenizer_v2(request, raw_request)</code>  <code>async</code>","text":"<p>Tokenizes a text</p> <p>The request should be a JSON object with the following fields:</p> <p>Completions : - model : ID of the model to use - prompt : The text to tokenize - add_special_tokens : Add a special tokens to the begin (optional, default value : <code>true</code>)</p> <p>Chat Completions: - model : ID of the model to use - messages: The texts to tokenize - add_special_tokens : Add a special tokens to the begin (optional, default value : <code>false</code>) - add_generation_prompt : Add generation prompt's model in decode response (optional, default value : <code>true</code>)</p> Source code in <code>happy_vllm/routers/functional.py</code> <pre><code>@router.post(\"/v2/tokenizer\", response_model=vllm_protocol.TokenizeResponse, dependencies=[Depends(validate_json_request)])\n@with_cancellation\nasync def tokenizer_v2(request: Annotated[vllm_protocol.TokenizeRequest,\n        Body(openapi_examples=request_openapi_examples[\"vllm_tokenizer\"])],\n        raw_request: Request\n    ):\n    \"\"\"Tokenizes a text\n\n    The request should be a JSON object with the following fields:\n\n    Completions :\n    - model : ID of the model to use\n    - prompt : The text to tokenize\n    - add_special_tokens : Add a special tokens to the begin (optional, default value : `true`)\n\n    Chat Completions:\n    - model : ID of the model to use\n    - messages: The texts to tokenize\n    - add_special_tokens : Add a special tokens to the begin (optional, default value : `false`)\n    - add_generation_prompt : Add generation prompt's model in decode response (optional, default value : `true`)\n    \"\"\"\n    model: Model = RESOURCES[RESOURCE_MODEL]\n    generator = await model.openai_serving_tokenization.create_tokenize(request, raw_request)\n    if isinstance(generator, vllm_protocol.ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    else:\n        if not isinstance(generator, vllm_protocol.TokenizeResponse):\n            raise TypeError(\"Expected generator to be an instance of vllm_protocol.TokenizeResponse\")\n        return JSONResponse(content=generator.model_dump())\n</code></pre>"},{"location":"reference/happy_vllm/routers/functional/#happy_vllm.routers.functional.verify_request","title":"<code>verify_request(request)</code>","text":"<p>Parses the sampling parameters to check if any combination will break the app</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code> (Union[ChatCompletionRequest, CompletionRequest]</code> <p>The request to verify</p> required <p>Returns:     None</p> Source code in <code>happy_vllm/routers/functional.py</code> <pre><code>def verify_request(request: Union[\n    vllm_protocol.ChatCompletionRequest,\n    vllm_protocol.CompletionRequest]) -&gt; None:\n    \"\"\"Parses the sampling parameters to check if any combination will break the app\n\n    Args:\n        request  (Union[ChatCompletionRequest, CompletionRequest]): The request to verify\n    Returns:\n        None\n    \"\"\"\n    status_code = 422\n    detail = None\n    if request.echo and request.stream:\n        detail=\"Use both echo and stream breaks backend\"\n    if request.temperature is not None and request.top_p is not None:\n        if request.temperature == 0 and request.top_p == 0:\n            detail=f\"Use temperature and top_p equal to 0 breaks the model\"\n    if request.temperature and request.top_k:\n        if request.temperature &gt; 2 and request.top_k == 1:\n            detail=f\"Use temperature with high value: {request.temperature} and top_k equals to 1 : {request.top_k} breaks the model\"\n    if request.top_p and request.top_k:\n        if request.top_p == 1 and request.top_k == 1:\n            detail=f\"Use top_p and top_k equal to 1 breaks the model\"\n    if request.max_tokens and request.min_tokens:\n        if request.max_tokens &lt; request.min_tokens:\n            detail=f\"Use max_tokens: {request.max_tokens} less than min_tokens : {request.min_tokens} breaks the model\"\n    if detail :\n        raise HTTPException(\n            status_code=status_code, \n            detail=detail\n        )\n</code></pre>"},{"location":"reference/happy_vllm/routers/technical/","title":"technical","text":""},{"location":"reference/happy_vllm/routers/technical/#happy_vllm.routers.technical.get_liveness","title":"<code>get_liveness()</code>  <code>async</code>","text":"<p>Liveness probe for k8s</p> Source code in <code>happy_vllm/routers/technical.py</code> <pre><code>@router.get(\n    \"/liveness\",\n    response_model=technical_schema.ResponseLiveness,\n    name=\"liveness\",\n    tags=[\"technical\"],\n)\nasync def get_liveness() -&gt; technical_schema.ResponseLiveness:\n    \"\"\"Liveness probe for k8s\"\"\"\n    liveness_msg = technical_schema.ResponseLiveness(alive=\"ok\")\n    return liveness_msg\n</code></pre>"},{"location":"reference/happy_vllm/routers/technical/#happy_vllm.routers.technical.get_readiness","title":"<code>get_readiness()</code>  <code>async</code>","text":"<p>Readiness probe for k8s</p> Source code in <code>happy_vllm/routers/technical.py</code> <pre><code>@router.get(\n    \"/readiness\",\n    response_model=technical_schema.ResponseReadiness,\n    name=\"readiness\",\n    tags=[\"technical\"],\n)\nasync def get_readiness() -&gt; technical_schema.ResponseReadiness:\n    \"\"\"Readiness probe for k8s\"\"\"\n    model: Model = RESOURCES.get(RESOURCE_MODEL)\n\n    if model and model.is_model_loaded():\n        return technical_schema.ResponseReadiness(ready=\"ok\")\n    else:\n        return technical_schema.ResponseReadiness(ready=\"ko\")\n</code></pre>"},{"location":"reference/happy_vllm/routers/technical/#happy_vllm.routers.technical.info","title":"<code>info()</code>  <code>async</code>","text":"<p>Rest resource for info</p> Source code in <code>happy_vllm/routers/technical.py</code> <pre><code>@router.get(\n    \"/v1/info\",\n    response_model=technical_schema.ResponseInformation,\n    name=\"information\",\n    tags=[\"technical\"],\n)\nasync def info() -&gt; technical_schema.ResponseInformation:\n    \"\"\"Rest resource for info\"\"\"\n    try :\n        model: Model = RESOURCES[RESOURCE_MODEL]\n    except KeyError:\n        raise KeyError(f\"Key : {RESOURCE_MODEL} not found\")\n\n    return technical_schema.ResponseInformation(\n        application=model.app_name,\n        version=utils.get_package_version(),\n        model_name=model._model_conf.get(\"model_name\", \"?\"),\n        vllm_version=utils.get_vllm_version(),\n        truncation_side=model.original_truncation_side,\n        max_length=model.max_model_len,\n        extra_information=model.extra_information\n    )\n</code></pre>"},{"location":"reference/happy_vllm/routers/schemas/","title":"schemas","text":""},{"location":"reference/happy_vllm/routers/schemas/functional/","title":"functional","text":"<p>Functional schemas</p>"},{"location":"reference/happy_vllm/routers/schemas/technical/","title":"technical","text":"<p>Technical schemas</p>"},{"location":"reference/happy_vllm/routers/schemas/technical/#happy_vllm.routers.schemas.technical.ResponseInformation","title":"<code>ResponseInformation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Return object for info resource</p> Source code in <code>happy_vllm/routers/schemas/technical.py</code> <pre><code>class ResponseInformation(BaseModel):\n    \"\"\"Return object for info resource\"\"\"\n\n    application: str = Field(None, title=\"Application name\")\n    version: str = Field(None, title=\"Application version\")\n    vllm_version: str = Field(None, title=\"Version of vLLM\")\n    model_name: str = Field(None, title=\"Model name\")\n    truncation_side: str = Field(None, title=\"Truncation side\")\n    max_length : int = Field(None, title=\"Max length\")\n    extra_information: dict = Field(None, title=\"Extra information\")\n    model_config = {\n        \"json_schema_extra\": {\n            \"examples\": [\n                response_examples[\"information\"]\n            ]\n        }\n    }\n</code></pre>"},{"location":"reference/happy_vllm/routers/schemas/technical/#happy_vllm.routers.schemas.technical.ResponseLiveness","title":"<code>ResponseLiveness</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Return object for liveness probe</p> Source code in <code>happy_vllm/routers/schemas/technical.py</code> <pre><code>class ResponseLiveness(BaseModel):\n    \"\"\"Return object for liveness probe\"\"\"\n\n    alive: str = Field(None, title=\"Alive message\")\n    model_config = {\n        \"json_schema_extra\": {\n            \"examples\": [\n                response_examples[\"liveness\"]\n            ]\n        }\n    }\n</code></pre>"},{"location":"reference/happy_vllm/routers/schemas/technical/#happy_vllm.routers.schemas.technical.ResponseReadiness","title":"<code>ResponseReadiness</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Return object for readiness probe</p> Source code in <code>happy_vllm/routers/schemas/technical.py</code> <pre><code>class ResponseReadiness(BaseModel):\n    \"\"\"Return object for readiness probe\"\"\"\n\n    ready: str = Field(None, title=\"Ready message\")\n    model_config = {\n        \"json_schema_extra\": {\n            \"examples\": [\n                response_examples[\"readiness\"]\n            ]\n        }\n    }\n</code></pre>"},{"location":"reference/happy_vllm/routers/schemas/utils/","title":"utils","text":""},{"location":"reference/happy_vllm/routers/schemas/utils/#happy_vllm.routers.schemas.utils.NumpyArrayEncoder","title":"<code>NumpyArrayEncoder</code>","text":"<p>               Bases: <code>JSONEncoder</code></p> <p>JSONEncoder to store python dict or list containing numpy arrays</p> Source code in <code>happy_vllm/routers/schemas/utils.py</code> <pre><code>class NumpyArrayEncoder(json.JSONEncoder):\n    \"\"\"JSONEncoder to store python dict or list containing numpy arrays\"\"\"\n\n    def default(self, obj: Any) -&gt; Any:\n        \"\"\"Transform numpy arrays into JSON serializable object such as list\n        see : https://docs.python.org/3/library/json.html#json.JSONEncoder.default\n        \"\"\"\n\n        # numpy.ndarray have dtype, astype and tolist attribute and methods that we want\n        # to use to convert their element into JSON serializable objects\n        if hasattr(obj, \"dtype\") and hasattr(obj, \"astype\") and hasattr(obj, \"tolist\"):\n\n            if np.issubdtype(obj.dtype, np.integer):\n                return obj.astype(int).tolist()\n            elif np.issubdtype(obj.dtype, np.number):\n                return obj.astype(float).tolist()\n            else:\n                return obj.tolist()\n\n        # sets are not json serializable\n        elif isinstance(obj, set):\n            return list(obj)\n\n        return json.JSONEncoder.default(self, obj)\n</code></pre>"},{"location":"reference/happy_vllm/routers/schemas/utils/#happy_vllm.routers.schemas.utils.NumpyArrayEncoder.default","title":"<code>default(obj)</code>","text":"<p>Transform numpy arrays into JSON serializable object such as list see : https://docs.python.org/3/library/json.html#json.JSONEncoder.default</p> Source code in <code>happy_vllm/routers/schemas/utils.py</code> <pre><code>def default(self, obj: Any) -&gt; Any:\n    \"\"\"Transform numpy arrays into JSON serializable object such as list\n    see : https://docs.python.org/3/library/json.html#json.JSONEncoder.default\n    \"\"\"\n\n    # numpy.ndarray have dtype, astype and tolist attribute and methods that we want\n    # to use to convert their element into JSON serializable objects\n    if hasattr(obj, \"dtype\") and hasattr(obj, \"astype\") and hasattr(obj, \"tolist\"):\n\n        if np.issubdtype(obj.dtype, np.integer):\n            return obj.astype(int).tolist()\n        elif np.issubdtype(obj.dtype, np.number):\n            return obj.astype(float).tolist()\n        else:\n            return obj.tolist()\n\n    # sets are not json serializable\n    elif isinstance(obj, set):\n        return list(obj)\n\n    return json.JSONEncoder.default(self, obj)\n</code></pre>"}]}