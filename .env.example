### All variables present in this .env are case insensitive. The "-" character present in the cli args is replaced by an underscore "_"  
# The specified values in the following examples are the default values

### Happy_vLLM log settings ###

# LOG_LEVEL="INFO"

### Application settings ###

# APP_NAME="happy_vllm"
# API_ENDPOINT_PREFIX=""
# HOST="127.0.0.1"
# PORT=5000
# EXPLICIT_ERRORS=true
# ALLOW_CREDENTIALS=false
# ALLOWED_ORIGINS=["*"]
# ALLOWED_METHODS=["*"]
# ALLOWED_HEADERS=["*"]
# UVICORN_LOG_LEVEL='info'
# SSL_KEYFILE=None
# SSL_CERTFILE=None
# SSL_CA_CERTS=None
# DEFAULT_SSL_CERT_REQS=0
# ROOT_PATH=None
# LORA_MODULES=None
# CHAT_TEMPLATE=None
# RESPONSE_ROLE="assistant"
# WITH_LAUNCH_ARGUMENTS=false
# MAX_LOG_LEN=None
# PROMPT_ADAPTERS=None
# RETURN_TOKENS_AS_TOKEN_IDS=false
# DISABLE_FRONTEND_MULTIPROCESSING=false

### Model settings ###

# MODEL='facebook/opt-125m'
# MODEL_NAME="?"
### You can also put any args from vllm concerning the model defined in (https://github.com/vllm-project/vllm/blob/main/vllm/engine/arg_utils.py) such
### as TRUST_REMOTE_CODE, DTYPE, MAX_MODEL_LEN, GPU_MEMORY_UTILIZATION, ...